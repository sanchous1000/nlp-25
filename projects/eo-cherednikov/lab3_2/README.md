# Лабораторная работа №3.2 (Тематическое моделирование)

## Описание

В этой лабораторной работе реализуется задача тематического моделирования текстов с использованием модели LDA. Работа основана на матрице "термин-документ", полученной в результате выполнения второй лабораторной работы.

## Используемые данные

Для экспериментов используются:
- Матрица "термин-документ", построенная с использованием CountVectorizer
- Размеченные корпуса текстов из директории `annotated-corpus`, содержащей обучающие и тестовые данные, разделенные по классам

## Модели и методы

### Основная модель:
- LDA - вероятностная модель для обнаружения скрытых тем в коллекции документов

### Метрики оценки:
- Perplexity
- R-squared
- Топ-10 ключевых слов для каждой темы
- Вероятности принадлежности документов к темам

## Эксперименты

### 1. Варьирование количества тем
Сравнение эффективности LDA модели при различном количестве тем: 2, 4, 5, 10, 20, 40.

### 2. Оценка perplexity
Анализ зависимости perplexity от количества тем для определения оптимального числа тем.

### 3. Анализ ключевых слов
Извлечение топ-10 ключевых слов для каждой темы в каждом эксперименте.

### 4. Вероятности принадлежности документов к темам
- Вычисление вероятностей принадлежности документов к темам для каждого эксперимента
- Сохранение вероятностей в формате `.tsv`
- Определение документов с самой высокой вероятностью принадлежности к каждой теме

### 5. Полиномиальная аппроксимация
Построение полиномиальной аппроксимации зависимости perplexity от количества тем с использованием метрики R-squared.

### 6. Влияние количества итераций
Анализ влияния количества итераций обучения (в 2 раза больше и в 2 раза меньше исходного значения) на качество модели.

## Результаты

### Основные выводы:
- Оптимальное количество тем: 2 (perplexity = 4395.6404)
- Лучшая полиномиальная аппроксимация: 4-й степени с r² = 0.9998
- Оптимальное количество итераций: 20 (perplexity = 4395.6404)

### Значения perplexity для разных количеств тем:
- 2 темы: 4504.1146
- 4 тем: 5122.0393
- 5 тем: 5131.7267
- 10 тем: 6281.7912
- 20 тем: 9810.2582
- 40 тем: 17143.7575

## Визуализации

- Зависимость perplexity от количества тем

![perplexity](/assets/1.png)

- График полиномиальной аппроксимации

![poly](/assets/2.png)

## Заключение

Было установлено, что минимальная perplexity достигается при двух темах, что может указывать на оптимальное количество тем для данного набора данных. Полиномиальная аппроксимация показала высокую степень соответствия (r² = 0.9998). Анализ влияния количества итераций показал, что оптимальное количество итераций составляет 20 для двух тем. Эксперименты также позволили определить ключевые слова для каждой темы и документы с наибольшей вероятностью принадлежности к каждой теме.