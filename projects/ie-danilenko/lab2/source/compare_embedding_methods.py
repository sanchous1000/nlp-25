"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (GloVe)
–∏ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (PCA).

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
"""

import numpy as np
import argparse
import pickle
from pathlib import Path
from text_to_glove import (
    initialize_glove_model,
    get_device
)
from text_to_tfidf import load_vocabulary, load_term_document_matrix
from demonstrate_glove_similarity import cosine_distance, get_word_vector
from apply_pca_to_basic_vectors import (
    text_to_frequency_vector,
    text_to_onehot_matrix,
    onehot_matrix_to_vector,
    text_to_frequency_matrix,
    frequency_matrix_to_vector,
    text_to_tfidf_vector
)


def get_word_vector_basic(
    word,
    vocabulary,
    method,
    pca_model=None,
    num_docs=None,
    term_doc_counts=None
):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –±–∞–∑–æ–≤—ã–º –º–µ—Ç–æ–¥–æ–º —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º PCA.
    
    Args:
        word: –°–ª–æ–≤–æ
        vocabulary: –°–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω -> –∏–Ω–¥–µ–∫—Å
        method: –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('frequency', 'onehot', 'frequency_matrix', 'tfidf')
        pca_model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å PCA (–µ—Å–ª–∏ None, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω—ã–π –≤–µ–∫—Ç–æ—Ä)
        num_docs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–ª—è TF-IDF)
        term_doc_counts: –°–ª–æ–≤–∞—Ä—å term_index -> –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–ª—è TF-IDF)
        
    Returns:
        –í–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ –∏–ª–∏ None, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    """
    word_lower = word.lower()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    text = word_lower
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞
    if method == 'frequency':
        vector = text_to_frequency_vector(text, vocabulary)
    elif method == 'onehot':
        matrix = text_to_onehot_matrix(text, vocabulary)
        vector = onehot_matrix_to_vector(matrix, method='mean')
    elif method == 'frequency_matrix':
        matrix = text_to_frequency_matrix(text, vocabulary)
        vector = frequency_matrix_to_vector(matrix, method='mean')
    elif method == 'tfidf':
        if num_docs is None or term_doc_counts is None:
            raise ValueError("–î–ª—è TF-IDF –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã num_docs –∏ term_doc_counts")
        vector = text_to_tfidf_vector(text, vocabulary, num_docs, term_doc_counts)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º PCA, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
    if pca_model is not None:
        vector = pca_model.transform(vector.reshape(1, -1))[0]
    
    return vector


def load_pca_model(pca_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å PCA –∏–∑ —Ñ–∞–π–ª–∞.
    
    Args:
        pca_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ PCA
        
    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å PCA
    """
    with open(pca_path, 'rb') as f:
        pca_model = pickle.load(f)
    return pca_model


def evaluate_word_pairs(
    word_pairs,
    glove_model,
    basic_methods,
    verbose=True
):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–∞—Ä—ã —Å–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
    
    Args:
        word_pairs: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2, –∫–∞—Ç–µ–≥–æ—Ä–∏—è)
        glove_model: –ú–æ–¥–µ–ª—å GloVe
        basic_methods: –°–ª–æ–≤–∞—Ä—å {–º–µ—Ç–æ–¥: (pca_model, vocabulary, num_docs, term_doc_counts)}
        verbose: –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {–º–µ—Ç–æ–¥: {–∫–∞—Ç–µ–≥–æ—Ä–∏—è: [—Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è]}}
    """
    results = {}
    
    # –î–æ–±–∞–≤–ª—è–µ–º GloVe –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results['glove'] = {'close': [], 'distant': []}
    
    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã
    for method_name in basic_methods.keys():
        results[method_name] = {'close': [], 'distant': []}
    
    if verbose:
        print(f"\n–û—Ü–µ–Ω–∫–∞ {len(word_pairs)} –ø–∞—Ä —Å–ª–æ–≤...")
    
    for word1, word2, category in word_pairs:
        # GloVe
        vec1_glove = get_word_vector(word1, glove_model)
        vec2_glove = get_word_vector(word2, glove_model)
        
        if vec1_glove is not None and vec2_glove is not None:
            dist_glove = cosine_distance(vec1_glove, vec2_glove)
            results['glove'][category].append(dist_glove)
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã
        for method_name, (pca_model, vocabulary, num_docs, term_doc_counts) in basic_methods.items():
            vec1_basic = get_word_vector_basic(
                word1, vocabulary, method_name, pca_model, num_docs, term_doc_counts
            )
            vec2_basic = get_word_vector_basic(
                word2, vocabulary, method_name, pca_model, num_docs, term_doc_counts
            )
            
            if vec1_basic is not None and vec2_basic is not None:
                dist_basic = cosine_distance(vec1_basic, vec2_basic)
                results[method_name][category].append(dist_basic)
    
    return results


def compare_methods(
    glove_model_path,
    pca_models_dir,
    vocab_path,
    matrix_path,
    output_dir=None
):
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
    
    Args:
        glove_model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ GloVe
        pca_models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—è–º–∏ PCA
        vocab_path: –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é
        matrix_path: –ü—É—Ç—å –∫ –º–∞—Ç—Ä–∏—Ü–µ "—Ç–µ—Ä–º–∏–Ω-–¥–æ–∫—É–º–µ–Ω—Ç"
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    print("=" * 80)
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    print("=" * 80)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å GloVe
    print("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ GloVe...")
    device = get_device()
    glove_model, word_to_id = initialize_glove_model(
        model_path=glove_model_path,
        device=device,
        retrain=False
    )
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è GloVe: {len(word_to_id)}")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {glove_model.embedding_dim}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    print("\n2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –∏ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤...")
    vocabulary = load_vocabulary(vocab_path)
    num_docs, term_doc_counts = load_term_document_matrix(matrix_path)
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vocabulary)}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {num_docs}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ PCA
    print("\n3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π PCA...")
    basic_methods = {}
    
    method_configs = {
        'frequency': 'pca_frequency.pkl',
        'onehot': 'pca_onehot.pkl',
        'frequency_matrix': 'pca_frequency_matrix.pkl',
        'tfidf': 'pca_tfidf.pkl',
    }
    
    for method_name, pca_filename in method_configs.items():
        pca_path = pca_models_dir / pca_filename
        if pca_path.exists():
            pca_model = load_pca_model(pca_path)
            basic_methods[method_name] = (pca_model, vocabulary, num_docs, term_doc_counts)
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å PCA –¥–ª—è –º–µ—Ç–æ–¥–∞ '{method_name}'")
        else:
            print(f"   ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å PCA –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {pca_path}")
    
    if not basic_methods:
        print("\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ PCA! –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ apply_pca_to_basic_vectors.py")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã —Å–ª–æ–≤
    print("\n4. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø–∞—Ä —Å–ª–æ–≤...")
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –ø–∞—Ä—ã
    close_pairs = [
        ('cat', 'tiger', 'close'),
        ('cat', 'feline', 'close'),
        ('president', 'leader', 'close'),
        ('president', 'chief', 'close'),
        ('company', 'corporation', 'close'),
        ('company', 'business', 'close'),
        ('war', 'battle', 'close'),
        ('war', 'conflict', 'close'),
        ('software', 'program', 'close'),
        ('software', 'application', 'close'),
    ]
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –¥–∞–ª–µ–∫–∏–µ –ø–∞—Ä—ã
    distant_pairs = [
        ('cat', 'sentence', 'distant'),
        ('cat', 'computer', 'distant'),
        ('president', 'animal', 'distant'),
        ('president', 'food', 'distant'),
        ('company', 'nature', 'distant'),
        ('company', 'music', 'distant'),
        ('war', 'peace', 'distant'),
        ('war', 'love', 'distant'),
        ('software', 'animal', 'distant'),
        ('software', 'food', 'distant'),
    ]
    
    all_pairs = close_pairs + distant_pairs
    print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏—Ö –ø–∞—Ä: {len(close_pairs)}")
    print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –¥–∞–ª–µ–∫–∏—Ö –ø–∞—Ä: {len(distant_pairs)}")
    print(f"   –í—Å–µ–≥–æ –ø–∞—Ä: {len(all_pairs)}")
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–∞—Ä—ã
    print("\n5. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π...")
    results = evaluate_word_pairs(all_pairs, glove_model, basic_methods, verbose=True)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n6. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    print("\n" + "=" * 80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø –ú–ï–¢–û–î–û–í")
    print("=" * 80)
    
    method_stats = {}
    
    for method_name in ['glove'] + list(basic_methods.keys()):
        if method_name not in results:
            continue
        
        close_distances = results[method_name]['close']
        distant_distances = results[method_name]['distant']
        
        if not close_distances or not distant_distances:
            continue
        
        avg_close = np.mean(close_distances)
        avg_distant = np.mean(distant_distances)
        std_close = np.std(close_distances)
        std_distant = np.std(distant_distances)
        
        # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Å—Ä–µ–¥–Ω–∏–º–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        separation = avg_distant - avg_close
        
        method_stats[method_name] = {
            'avg_close': avg_close,
            'avg_distant': avg_distant,
            'std_close': std_close,
            'std_distant': std_distant,
            'separation': separation
        }
        
        print(f"\n{method_name.upper()}:")
        print(f"  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ —Å–ª–æ–≤–∞:")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {avg_close:.6f} ¬± {std_close:.6f}")
        print(f"    –ú–∏–Ω–∏–º—É–º: {min(close_distances):.6f}, –ú–∞–∫—Å–∏–º—É–º: {max(close_distances):.6f}")
        print(f"  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –¥–∞–ª–µ–∫–∏–µ —Å–ª–æ–≤–∞:")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {avg_distant:.6f} ¬± {std_distant:.6f}")
        print(f"    –ú–∏–Ω–∏–º—É–º: {min(distant_distances):.6f}, –ú–∞–∫—Å–∏–º—É–º: {max(distant_distances):.6f}")
        print(f"  –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ (–¥–∞–ª–µ–∫–∏–µ - –±–ª–∏–∑–∫–∏–µ): {separation:.6f}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
    print("\n" + "=" * 80)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê")
    print("=" * 80)
    print(f"{'–ú–µ—Ç–æ–¥':<20} {'–°—Ä–µ–¥–Ω–µ–µ (–±–ª–∏–∑–∫–∏–µ)':<20} {'–°—Ä–µ–¥–Ω–µ–µ (–¥–∞–ª–µ–∫–∏–µ)':<20} {'–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ':<15}")
    print("-" * 80)
    
    for method_name, stats in sorted(method_stats.items(), key=lambda x: x[1]['separation'], reverse=True):
        print(f"{method_name:<20} {stats['avg_close']:<20.6f} {stats['avg_distant']:<20.6f} {stats['separation']:<15.6f}")
    
    # –í—ã–≤–æ–¥—ã
    print("\n" + "=" * 80)
    print("–í–´–í–û–î–´")
    print("=" * 80)
    
    # –ù–∞—Ö–æ–¥–∏–º –º–µ—Ç–æ–¥ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º
    best_method = max(method_stats.items(), key=lambda x: x[1]['separation'])
    
    print(f"\n‚úÖ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥ –ø–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –±–ª–∏–∑–∫–∏—Ö –∏ –¥–∞–ª–µ–∫–∏—Ö —Å–ª–æ–≤: {best_method[0].upper()}")
    print(f"   –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {best_method[1]['separation']:.6f}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å GloVe
    if 'glove' in method_stats:
        glove_separation = method_stats['glove']['separation']
        print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å GloVe (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {glove_separation:.6f}):")
        
        for method_name, stats in method_stats.items():
            if method_name == 'glove':
                continue
            
            diff = stats['separation'] - glove_separation
            percent_diff = (diff / glove_separation) * 100 if glove_separation > 0 else 0
            
            if diff > 0:
                print(f"  {method_name}: –ª—É—á—à–µ –Ω–∞ {diff:.6f} ({percent_diff:+.2f}%)")
            elif diff < 0:
                print(f"  {method_name}: —Ö—É–∂–µ –Ω–∞ {abs(diff):.6f} ({abs(percent_diff):+.2f}%)")
            else:
                print(f"  {method_name}: –æ–¥–∏–Ω–∞–∫–æ–≤–æ")
    
    # –û–±—â–∏–π –≤—ã–≤–æ–¥
    print(f"\nüìù –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
    print(f"   –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏—Ö –∏ –¥–∞–ª–µ–∫–∏—Ö –ø–∞—Ä —Å–ª–æ–≤:")
    
    if best_method[0] == 'glove':
        print(f"   - –ú–µ—Ç–æ–¥ GloVe (–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        print(f"   - GloVe –ª—É—á—à–µ —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –∏ –¥–∞–ª–µ–∫–∏–µ —Å–ª–æ–≤–∞")
        print(f"   - –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å PCA –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ —É—Å—Ç—É–ø–∞—é—Ç GloVe")
    else:
        print(f"   - –ú–µ—Ç–æ–¥ {best_method[0]} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        print(f"   - –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å PCA –º–æ–≥—É—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ GloVe –Ω–∞ –¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
        if 'glove' in method_stats:
            print(f"   - GloVe –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {glove_separation:.6f}")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏'
    )
    parser.add_argument(
        '--glove-model',
        type=str,
        default=None,
        help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ GloVe (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: output/glove_model.pkl)'
    )
    parser.add_argument(
        '--pca-dir',
        type=str,
        default=None,
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—è–º–∏ PCA (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: output/)'
    )
    parser.add_argument(
        '--vocab',
        type=str,
        default=None,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–ª–æ–≤–∞—Ä—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: output/vocabulary.json)'
    )
    parser.add_argument(
        '--matrix',
        type=str,
        default=None,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–∞—Ç—Ä–∏—Ü—ã "—Ç–µ—Ä–º–∏–Ω-–¥–æ–∫—É–º–µ–Ω—Ç" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: output/term_document_matrix.json)'
    )
    
    args = parser.parse_args()
    
    # –ü—É—Ç–∏
    base_dir = Path(__file__).parent
    
    if args.glove_model:
        glove_model_path = Path(args.glove_model)
        if not glove_model_path.is_absolute():
            glove_model_path = base_dir / glove_model_path
    else:
        glove_model_path = base_dir / "output" / "glove_model.pkl"
    
    if args.pca_dir:
        pca_models_dir = Path(args.pca_dir)
        if not pca_models_dir.is_absolute():
            pca_models_dir = base_dir / pca_models_dir
    else:
        pca_models_dir = base_dir / "output"
    
    if args.vocab:
        vocab_path = Path(args.vocab)
        if not vocab_path.is_absolute():
            vocab_path = base_dir / vocab_path
    else:
        vocab_path = base_dir / "output" / "vocabulary.json"
    
    if args.matrix:
        matrix_path = Path(args.matrix)
        if not matrix_path.is_absolute():
            matrix_path = base_dir / matrix_path
    else:
        matrix_path = base_dir / "output" / "term_document_matrix.json"
    
    compare_methods(
        glove_model_path=glove_model_path,
        pca_models_dir=pca_models_dir,
        vocab_path=vocab_path,
        matrix_path=matrix_path
    )

