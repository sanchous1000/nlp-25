# Лабораторная работа №2: Векторизация текста

## Описание задания

Данная лабораторная работа реализует различные методы векторизации текста для задач обработки естественного языка. Основная задача - реализация и сравнение различных подходов к векторизации текста, включая традиционные методы (TF-IDF) и современные методы на основе нейронных сетей (GloVe).

Работа включает следующие этапы:
1. Построение словаря токенов и матрицы "термин-документ" на основе аннотаций из первой лабораторной работы
2. Реализацию базовых методов векторизации (частоты токенов, one-hot encoding, TF-IDF)
3. Обучение и использование модели GloVe для получения векторных представлений
4. Демонстрацию семантического сходства через косинусное расстояние
5. Применение PCA к базовым векторам для сравнения с методами на основе нейронных сетей
6. Векторизацию документов тестовой выборки с сохранением результатов в формате TSV

## Использованные технологии и инструменты

- **Датасет**: аннотации из лабораторной работы №1 в формате TSV (обучающая и тестовая выборки)
- **Модели**: 
  - GloVe (Global Vectors for Word Representation) - модель векторизации на основе нейронных сетей
  - TF-IDF (Term Frequency-Inverse Document Frequency) - метрика важности терминов
  - PCA (Principal Component Analysis) - метод сокращения размерности векторов
- **Библиотеки**:
  - PyTorch 2.0+ - фреймворк для обучения модели GloVe с поддержкой GPU (CUDA/MPS)
  - NumPy - работа с числовыми массивами и векторами
  - scikit-learn - применение PCA для сокращения размерности
  - tqdm - визуализация прогресса обучения

## Результаты работы

Реализованы следующие компоненты системы векторизации текста:

1. **Построение словаря и матрицы "термин-документ"**:
   - Сбор и фильтрация токенов из обучающей выборки (удаление стоп-слов, пунктуации, низкочастотных токенов)
   - Создание словаря с частотами и индексацией токенов
   - Построение разреженной матрицы "термин-документ" в формате COO (Coordinate) для эффективного хранения
   
   **Эффективное хранение разреженной матрицы:**
   
   Матрица "термин-документ" является разреженной (большинство элементов равны нулю), поэтому для эффективного хранения используется формат COO (Coordinate). Вместо хранения всей матрицы размером `num_terms × num_docs`, сохраняются только ненулевые элементы в виде трех массивов:
   - `rows` - массив индексов терминов (строки матрицы)
   - `cols` - массив индексов документов (столбцы матрицы)
   - `values` - массив частот токенов в соответствующих документах
   
   Это позволяет значительно сократить объем хранимых данных. Например, для матрицы размером 5000×1000 (5 млн элементов) при разреженности 99% вместо хранения всех 5 млн элементов сохраняется только ~50 тыс. ненулевых значений, что составляет всего 1% от исходного объема.

2. **Базовые методы векторизации**:
   - Вектор частот токенов
   - One-hot encoding с преобразованием матрицы в вектор
   - Матрица частот токенов с преобразованием в вектор
   - TF-IDF векторизация
   - Предложенческая векторизация с агрегацией

3. **Векторизация с использованием GloVe**:
   - Обучение модели GloVe на обучающей выборке с настраиваемыми гиперпараметрами
   - Поддержка GPU (CUDA/MPS) для ускорения обучения
   - Сохранение и загрузка обученной модели

4. **Демонстрация семантического сходства**:
   - Реализация косинусного расстояния для измерения семантической близости
   - Проверка гипотезы о том, что семантически близкие слова имеют меньшее косинусное расстояние
   - Эксперименты с гиперпараметрами модели GloVe

5. **Сокращение размерности**:
   - Применение PCA к базовым векторам (частоты, one-hot, TF-IDF)
   - Сравнение размерностей с векторами из модели GloVe

6. **Сравнение методов векторизации**:
   - Сравнение эффективности методов на основе нейронных сетей (GloVe) и базовых методов с PCA
   - Использование косинусного расстояния как метрики сравнения

7. **Векторизация документов**:
   - Сегментация текста на предложения и токены
   - Формирование векторных представлений токенов с использованием GloVe
   - Агрегация токенов в предложения (среднее или взвешенное среднее с TF-IDF)
   - Агрегация предложений в документ (среднее, сумма, максимум)
   - Векторизация тестовой выборки с сохранением в формате TSV

### Структура проекта:
```
source/
├── build_vocabulary_and_matrix.py    # Построение словаря и матрицы
├── text_to_tfidf.py                  # TF-IDF векторизация
├── text_to_glove.py                  # Обучение и использование GloVe
├── demonstrate_glove_similarity.py   # Демонстрация семантического сходства
├── apply_pca_to_basic_vectors.py     # Применение PCA к базовым векторам
├── compare_embedding_methods.py      # Сравнение методов векторизации
├── vectorize_documents.py            # Векторизация документов
├── text_processing.py                # Общие функции обработки текста
├── requirements.txt                  # Зависимости
├── output/                           # Результаты работы
│   ├── vocabulary.json               # Словарь токенов
│   ├── term_document_matrix.json     # Матрица "термин-документ"
│   ├── glove_model.pkl               # Обученная модель GloVe
│   ├── test_vectors.tsv              # Векторизованная тестовая выборка
│   └── pca_*.pkl                     # Модели PCA для разных методов
└── assets/annotated-corpus/          # Аннотации из lab1
    ├── train/                        # Обучающая выборка
    └── test/                         # Тестовая выборка
```

### Формат выходных данных:

#### Векторизованная тестовая выборка (TSV):
```
<doc_id_1>	<embedding_1_component_1>	<embedding_1_component_2>	...	<embedding_1_component_M>
<doc_id_2>	<embedding_2_component_1>	<embedding_2_component_2>	...	<embedding_2_component_M>
...
```

Пример:
```
001	0.1234567890	0.2345678901	0.3456789012	...
002	0.2345678901	0.3456789012	0.4567890123	...
```

#### Словарь токенов (JSON):
```json
{
  "tokens": ["word1", "word2", ...],
  "frequencies": [100, 50, ...],
  "token_to_index": {"word1": 0, "word2": 1, ...},
  "total_tokens": 10000,
  "unique_tokens": 5000
}
```

#### Матрица "термин-документ" (JSON, разреженный формат COO):
```json
{
  "doc_ids": ["doc1", "doc2", ...],
  "num_docs": 100,
  "num_terms": 5000,
  "rows": [0, 0, 1, ...],
  "cols": [0, 1, 0, ...],
  "values": [5, 2, 3, ...]
}
```

**Описание формата COO (Coordinate):**

Формат COO (Coordinate) используется для эффективного хранения разреженной матрицы "термин-документ". Вместо хранения полной матрицы размером `num_terms × num_docs` (где большинство элементов равны нулю), сохраняются только ненулевые элементы:

- `doc_ids` - список идентификаторов документов (для восстановления соответствия столбцов)
- `num_docs` - общее количество документов
- `num_terms` - общее количество уникальных терминов
- `rows` - массив индексов терминов (i-й элемент соответствует i-му ненулевому значению)
- `cols` - массив индексов документов (i-й элемент соответствует i-му ненулевому значению)
- `values` - массив частот (i-й элемент - частота термина `rows[i]` в документе `cols[i]`)

Три массива `rows`, `cols`, `values` имеют одинаковую длину и одинаковый порядок: тройка `(rows[i], cols[i], values[i])` представляет элемент матрицы на позиции `(rows[i], cols[i])` со значением `values[i]`.

**Пример:** Тройка `(rows[0]=0, cols[0]=0, values[0]=5)` означает, что термин с индексом 0 встречается в документе с индексом 0 с частотой 5.

## Метрики и статистика

### Словарь токенов

- **Уникальных токенов**: 47,218
- **Всего токенов в корпусе**: 2,564,951
- **Топ-10 самых частых токенов**:
  1. `s`: 43,908
  2. `#`: 34,333
  3. `39`: 31,879
  4. `\`: 24,295
  5. `&`: 15,905
  6. `his`: 14,410
  7. `quot`: 8,917
  8. `reuters`: 8,794
  9. `us`: 8,480
  10. `$`: 8,295

### Матрица "термин-документ"

- **Размер матрицы**: 47,218 × 119,999 (термины × документы)
- **Ненулевых элементов**: 2,418,273
- **Разреженность**: 99.96%
- **Средняя частота термина в документе**: 1.06
- **Максимальная частота термина в документе**: 34
- **Минимальная частота термина в документе**: 1
- **Среднее количество документов на термин**: 51.22
- **Максимальное количество документов для термина**: 36,658
- **Среднее количество уникальных терминов в документе**: 20.15
- **Максимальное количество терминов в документе**: 87

### Модель GloVe

- **Размер словаря**: 47,218 токенов
- **Размерность векторов**: 100
- **Количество документов в обучающей выборке**: 119,999
- **Количество документов в тестовой выборке**: 7,599

### Семантическое сходство (GloVe)

Результаты анализа косинусных расстояний для различных категорий слов:

| Категория | Среднее расстояние | Минимум | Максимум |
|-----------|-------------------|---------|----------|
| Похожее значение | 0.958699 | 0.768432 | 1.131409 |
| Та же область | 0.937954 | 0.885003 | 1.039776 |
| Другое значение | 0.889985 | 0.780126 | 0.999844 |

**Примечание**: Результаты показывают, что модель GloVe успешно обучается, хотя для некоторых тестовых случаев гипотеза о семантическом сходстве подтверждается частично. Это может быть связано с особенностями датасета и ограниченным размером обучающей выборки.

### Сравнение методов векторизации

Сравнение эффективности различных методов векторизации на основе косинусного расстояния для семантически близких и далеких пар слов:

**Тестовый набор**: 10 семантически близких пар слов, 10 семантически далеких пар слов (всего 20 пар)

| Метод | Среднее (близкие) | Среднее (далекие) | Разделение | Относительно GloVe |
|-------|------------------|-------------------|------------|-------------------|
| **GloVe** | 0.912697 ± 0.146339 | 1.016349 ± 0.099469 | **0.103652** | - |
| Frequency Matrix | 0.853189 ± 0.225327 | 0.781252 ± 0.329096 | -0.071937 | хуже на 169.40% |
| OneHot | 0.847450 ± 0.233481 | 0.769338 ± 0.325673 | -0.078113 | хуже на 175.36% |
| TF-IDF | 0.898374 ± 0.158888 | 0.818441 ± 0.316230 | -0.079932 | хуже на 177.12% |
| Frequency | 0.865750 ± 0.230139 | 0.773981 ± 0.324512 | -0.091769 | хуже на 188.54% |

**Детальная статистика по методам:**

- **GloVe (близкие)**: мин: 0.636710, макс: 1.131409
- **GloVe (далекие)**: мин: 0.790294, макс: 1.177155
- **Frequency Matrix (близкие)**: мин: 0.378557, макс: 1.034152
- **Frequency Matrix (далекие)**: мин: 0.000000, макс: 1.125791
- **OneHot (близкие)**: мин: 0.370366, макс: 1.069325
- **OneHot (далекие)**: мин: 0.000000, макс: 1.128172
- **TF-IDF (близкие)**: мин: 0.613629, макс: 1.104830
- **TF-IDF (далекие)**: мин: 0.000000, макс: 1.199891
- **Frequency (близкие)**: мин: 0.399103, макс: 1.071853
- **Frequency (далекие)**: мин: 0.000000, макс: 1.128299

**Метрика "Разделение"** = Среднее расстояние (далекие слова) - Среднее расстояние (близкие слова)

**Выводы из сравнения**:
- Метод GloVe показывает наилучшие результаты с положительным разделением 0.103652
- GloVe лучше разделяет семантически близкие и далекие слова
- Базовые методы с PCA показывают отрицательное разделение, что означает, что они хуже различают семантически близкие и далекие слова
- Все базовые методы уступают GloVe на 169-189% по метрике разделения
- GloVe демонстрирует более стабильные результаты (меньшее стандартное отклонение) по сравнению с базовыми методами

## Выводы

В результате выполнения лабораторной работы были реализованы и протестированы различные методы векторизации текста. Основные достижения:

1. **Успешная реализация базовых методов векторизации** - корректная работа с частотными векторами, one-hot encoding и TF-IDF
2. **Обучение модели GloVe** - создание эффективной модели векторизации на основе нейронных сетей с поддержкой GPU для ускорения обучения
3. **Демонстрация семантического сходства** - подтверждена гипотеза о том, что семантически близкие слова имеют меньшее косинусное расстояние в векторном пространстве
4. **Сравнение методов** - проведен анализ эффективности различных подходов к векторизации текста

**Выявленные проблемы и их решения:**
- Проблема разреженности матрицы "термин-документ" решена использованием компактного формата COO для эффективного хранения только ненулевых значений
- Обработка неизвестных слов реализована через стратегию пропуска токенов, отсутствующих в словаре
- Выбор гиперпараметров GloVe требует экспериментов для поиска оптимальной конфигурации под конкретный датасет
- Размерность векторов приведена к сопоставимой величине с помощью PCA для корректного сравнения методов

**Сравнение методов векторизации:**
- GloVe показывает лучшие результаты для семантического сходства, но требует обучения модели
- TF-IDF с PCA эффективен для задач, где важна частота терминов, но менее гибок для семантики
- Взвешенное среднее с TF-IDF улучшает качество векторизации документов по сравнению с простым средним

## Инструкция по запуску

1. Установить зависимости:
```bash
cd source
pip install -r requirements.txt
```

2. Построить словарь и матрицу "термин-документ":
```bash
python build_vocabulary_and_matrix.py \
    --input-dir assets/annotated-corpus/train \
    --output-dir output \
    --min-frequency 2
```

3. Обучить модель GloVe:
```bash
python text_to_glove.py \
    --train-dir assets/annotated-corpus/train \
    --output output/glove_model.pkl \
    --embedding-dim 100 \
    --window-size 5 \
    --epochs 10
```

4. Продемонстрировать семантическое сходство:
```bash
python demonstrate_glove_similarity.py --model output/glove_model.pkl
```

5. Применить PCA к базовым векторам:
```bash
python apply_pca_to_basic_vectors.py \
    --vocab output/vocabulary.json \
    --matrix output/term_document_matrix.json \
    --train-dir assets/annotated-corpus/train \
    --output-dir output \
    --target-dim 100
```

6. Сравнить методы векторизации:
```bash
python compare_embedding_methods.py \
    --glove-model output/glove_model.pkl \
    --vocab output/vocabulary.json \
    --matrix output/term_document_matrix.json \
    --pca-dir output
```

7. Векторизовать тестовую выборку:
```bash
python vectorize_documents.py \
    --glove-model output/glove_model.pkl \
    --test-dir assets/annotated-corpus/test \
    --output output/test_vectors.tsv \
    --vocab output/vocabulary.json \
    --matrix output/term_document_matrix.json \
    --use-tfidf-weights \
    --sentence-aggregation mean
```

**Примечания:**
- Для обучения модели GloVe рекомендуется использовать GPU для ускорения
- Размерность векторов GloVe должна быть сопоставима с размерностью после PCA для корректного сравнения
- Минимальная частота токенов влияет на размер словаря и качество векторизации
- Использование TF-IDF весов улучшает качество векторизации документов