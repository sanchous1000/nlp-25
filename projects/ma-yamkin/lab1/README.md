# Лабораторная работа №1

## Описание задания

В лабораторной работе необходимо было провести токенизацию предложений, поступаемых на вход с учетом различных случаев обработки (например, почта, номер телефона, математическое выражение) с помощью регулярных выражений. После чего необходимо было провести стемминг и лемматизацию и сохранить результат работы в аннотацию.

## Использованные технологии и инструменты
- Датасет: Базовый новостной датасет
- Модели: SnowballStemmer, WordNetLemmatizer.
- Библиотеки: pandas, argparse, tqdm, time, nltk, pathlib, re.

## Результаты работы
В результате работы была реализована токенизация с помощью библиотеки re. Для токенизации были учтены несколько особых случаев: почта, номер телефона, обращения, математические выражения. После чего был проведен стемминг с помощью алгоритма SnowballStemmer. Лемматизация была проведена с помощью модели WordNetLemmatizer, она осуществлялась с учетом части речи. Обе модели были взяты из библиотеки nltk. 

Время кодирования тренировочного датасета и сохранения в аннотацию заняло около 6 минут. Время кодирования тестового датасета составило 25 секунд. 

Также был рассмотрен случай омонимии. A dove flew by. She dove into the pool. В результате лемматизации dove приняло разные формы: dove и dive, поскольку в разных контекстах оно является разной частью речи. Проверить данное утверждение запустив скрипт командой `python source/main.py --dataset 'manual'`. 

## Выводы
Итого:
1. Токенизация является алгоритмом который можно на базовом уровне реализовать с помощью регулярных выражений;
2. При осуществлении лемматизации важным моментом является часть речи. Потому что у одинаковых слов (пишутся одинаково) могут быть различные начальные формы. Это называется омонимией. В зависимости от контекста будут разные формы;
3. Токенизация занимает немного времени. На токенизацию, лемматизацию и стемминг всего датасета ушло чуть больше 6 минут.

## Инструкция по запуску
1. Установить зависимости: `pip install -r requirements.txt`
2. Загрузить датасет: [train.csv](https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv) и [test.csv](https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv) в папку lab1 c соответствующими названиями.
3. Запустить скрипт: `python source/main.py --dataset 'train'` для тренировочного датасета. Запустить скрипт: `python source/main.py --dataset 'test'` для тестового датасета.
