{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a6d96c",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3.1: Классификация текстов\n",
    "\n",
    "## Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3c42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    doc_ids = []\n",
    "    embeddings = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            doc_ids.append(parts[0])\n",
    "            embeddings.append(list(map(float, parts[1:])))\n",
    "\n",
    "    return doc_ids, np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df53bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_vectors(path):\n",
    "    embeddings = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            vector = list(map(float, parts[1:]))\n",
    "            embeddings.append(vector)\n",
    "\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa9ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_document_vectors(\"document_vectors.tsv\")\n",
    "\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc366ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7b0b5806ca4ec6b3a11db6b8df3029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\datasets--wangrongsheng--ag_news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c67c9599054466a9229bd1d0ccbb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f742de28eaf14643a3e4cbf9cf53dd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559ebebd9c7646d49120abd287bf0f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42ea3d7b85d4fbd949a05978368cd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wangrongsheng/ag_news\")\n",
    "labels = np.array(dataset[\"train\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b82715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 100\n",
      "Labels: 100\n"
     ]
    }
   ],
   "source": [
    "labels = labels[:embeddings.shape[0]]\n",
    "print(\"Embeddings:\", embeddings.shape[0])\n",
    "print(\"Labels:\", labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935476c8",
   "metadata": {},
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, num_classes):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        fp = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        fn = np.sum((y_pred != cls) & (y_true == cls))\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return (\n",
    "        np.mean(precisions),\n",
    "        np.mean(recalls),\n",
    "        np.mean(f1s)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b5232",
   "metadata": {},
   "source": [
    "## Эксперименты с SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X, y, kernel, max_iter):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = SVC(\n",
    "        kernel=kernel,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "    p, r, f1 = precision_recall_f1(y_test, y_pred, num_classes=4)\n",
    "\n",
    "    return {\n",
    "        \"kernel\": kernel,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1,\n",
    "        \"training_time\": training_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25b588f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'linear', 'max_iter': 200, 'accuracy': 0.85, 'precision': 0.3472222221473765, 'recall': 0.31862745093877354, 'f1': 0.3285714282734694, 'training_time': 0.0036818981170654297}\n",
      "{'kernel': 'linear', 'max_iter': 500, 'accuracy': 0.85, 'precision': 0.3472222221473765, 'recall': 0.31862745093877354, 'f1': 0.3285714282734694, 'training_time': 0.0009968280792236328}\n",
      "{'kernel': 'linear', 'max_iter': 1000, 'accuracy': 0.85, 'precision': 0.3472222221473765, 'recall': 0.31862745093877354, 'f1': 0.3285714282734694, 'training_time': 0.0009975433349609375}\n",
      "{'kernel': 'rbf', 'max_iter': 200, 'accuracy': 0.85, 'precision': 0.212499999989375, 'recall': 0.24999999998529412, 'f1': 0.22972972959313365, 'training_time': 0.0005142688751220703}\n",
      "{'kernel': 'rbf', 'max_iter': 500, 'accuracy': 0.85, 'precision': 0.212499999989375, 'recall': 0.24999999998529412, 'f1': 0.22972972959313365, 'training_time': 0.0009963512420654297}\n",
      "{'kernel': 'rbf', 'max_iter': 1000, 'accuracy': 0.85, 'precision': 0.212499999989375, 'recall': 0.24999999998529412, 'f1': 0.22972972959313365, 'training_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "kernels = [\"linear\", \"rbf\"]\n",
    "iterations = [200, 500, 1000]\n",
    "\n",
    "results = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    for it in iterations:\n",
    "        res = run_experiment(embeddings, labels, kernel, it)\n",
    "        results.append(res)\n",
    "        print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae31ad",
   "metadata": {},
   "source": [
    "## Отбрасывание размерностей эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dimensions(X, drop_ratio):\n",
    "    X_new = X.copy()\n",
    "    dim = X.shape[1]\n",
    "    drop_count = int(dim * drop_ratio)\n",
    "\n",
    "    indices = np.random.choice(dim, drop_count, replace=False)\n",
    "    X_new[:, indices] = 0.0\n",
    "\n",
    "    return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "778b09ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'linear', 'max_iter': 500, 'accuracy': 0.85, 'precision': 0.3472222221473765, 'recall': 0.31862745093877354, 'f1': 0.3285714282734694, 'training_time': 0.0009930133819580078, 'drop_ratio': 0.1}\n",
      "{'kernel': 'linear', 'max_iter': 500, 'accuracy': 0.85, 'precision': 0.3472222221473765, 'recall': 0.31862745093877354, 'f1': 0.3285714282734694, 'training_time': 0.0014445781707763672, 'drop_ratio': 0.3}\n",
      "{'kernel': 'linear', 'max_iter': 500, 'accuracy': 0.9, 'precision': 0.4736842102645429, 'recall': 0.3333333332908497, 'f1': 0.3611111108171296, 'training_time': 0.0015511512756347656, 'drop_ratio': 0.5}\n",
      "{'kernel': 'linear', 'max_iter': 500, 'accuracy': 0.85, 'precision': 0.212499999989375, 'recall': 0.24999999998529412, 'f1': 0.22972972959313365, 'training_time': 0.0004963874816894531, 'drop_ratio': 0.7}\n"
     ]
    }
   ],
   "source": [
    "drop_ratios = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "degradation_results = []\n",
    "\n",
    "for ratio in drop_ratios:\n",
    "    X_dropped = drop_dimensions(embeddings, ratio)\n",
    "    res = run_experiment(\n",
    "        X_dropped,\n",
    "        labels,\n",
    "        kernel=\"linear\",\n",
    "        max_iter=500\n",
    "    )\n",
    "    res[\"drop_ratio\"] = ratio\n",
    "    degradation_results.append(res)\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda)",
   "language": "python",
   "name": "conda_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
