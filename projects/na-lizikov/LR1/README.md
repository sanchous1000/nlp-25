# Лабораторная работа №1: Сегментация и аннотация текста

## Описание задания
Лабораторная работа посвящена предобработке текстовых данных с использованием методов NLP. Основные задачи:
- Реализация сегментации текста на предложения и токенизации с обработкой специальных случаев (URL, email, номера телефонов, адреса и т.д.)
- Разработка кастомного токенизатора для русского и английского текста
- Стемминг и лемматизация текста
- Создание аннотированного корпуса в формате TSV
- Анализ омонимии в тексте

Работа включает обработку двух датасетов (train.csv и test.csv), содержащих новостные статьи с метками категорий.

## Использованные технологии и инструменты
- **Датасеты:** Новостные статьи в формате CSV ([AG NEWS](https://huggingface.co/datasets/wangrongsheng/ag_news))
- **Модели:**
  - NLTK для токенизации предложений
  - SnowballStemmer для стемминга английских слов
  - WordNetLemmatizer для лемматизации
- **Библиотеки:**
  - Python 3.12.3
  - NLTK 3.8.1
  - re (регулярные выражения)
  - csv для работы с CSV-файлами
  - os, pathlib для работы с файловой системой

## Результаты работы
1. **Реализован AdvancedTokenizer** с поддержкой:
   - Сегментации предложений с учетом аббревиатур
   - Токенизации специальных паттернов (email, телефоны, адреса, формулы, смайлики)
   - Обработки омонимии и многозначных слов

2. **Создан аннотированный корпус** в формате TSV:
   - Обработано 119809 документов из train.csv
   - Обработано 7590 документов из test.csv
   - Структура файлов: `токен \t основа \t лемма`

3. **Анализ омонимии**:
   - Выявлены слова с потенциальной многозначностью (например: "bank" - банк/берег, "can" - мочь/консервировать)
   - Для каждого токена сохранены варианты лемматизации в зависимости от части речи

4. **Пример работы токенизатора**:
```
Текст: Привет! Мой email: abc@example.com, телефон: +7-901-000-00-00
Предложения: ['Привет!', 'Мой email: abc@example.com, телефон: +7-901-000-00-00']
Токены: ['Привет', '!']
Токены: ['Мой', 'email', ':', 'abc@example.com', ',', 'телефон', ':', '+7-901-000-00', '-', '00']
```

5. **Пример омонимии**:
```
Найдено 134 слов с потенциальной омонимией:
  bears: {'n': 'bear', 'v': 'bear'}
  looks: {'n': 'look', 'v': 'look'}
  has: {'n': 'ha', 'v': 'have'}
  plays: {'n': 'play', 'v': 'play'}
  bets: {'n': 'bet', 'v': 'bet'}
  stocks: {'n': 'stock', 'v': 'stock'}
  prices: {'n': 'price', 'v': 'price'}
  halts: {'n': 'halt', 'v': 'halt'}
  exports: {'n': 'export', 'v': 'export'}
  records: {'n': 'record', 'v': 'record'}
```

## Выводы
1. **Эффективность токенизатора**: Кастомный токенизатор успешно обрабатывает смешанные русско-английские тексты и специальные паттерны
2. **Проблемы лемматизации**: 
- WordNetLemmatizer ориентирован на английский язык, требует адаптации для русского
- Омонимия требует контекстного анализа для правильного определения леммы
3. **Масштабируемость**: Реализация эффективно обрабатывает большие объемы данных (~120k документов)
4. **Улучшения**: Для промышленного использования необходима:
- Поддержка большего количества языков
- Контекстная лемматизация
- Обработка неоднозначностей на уровне предложений

## Инструкция по запуску

### 1. Установка зависимостей
```bash
pip install -r requirements.txt
```
Файл requirements.txt:
```
nltk==3.8.1
```
### 2. Загрузка датасетов
Поместите файлы train.csv и test.csv в корневую директорию проекта.
### 3. Запуск обработки
Запустите Jupyter Notebook:
```
jupyter notebook LR1.ipynb
```
Или выполните скрипт напрямую через Python:
```
# Для запуска всей обработки выполните все ячейки ноутбука
# Основные шаги:
# 1. Импорт библиотек (ячейка 1)
# 2. Создание токенизатора (ячейка 2)
# 3. Тестирование (ячейка 3)
# 4. Обработка датасетов (ячейки 6-9)
```

### 4. Проверка результатов
После выполнения обработки проверьте:
- Количество созданных файлов в annotated-corpus/
- Пример аннотации через последнюю ячейку ноутбука
- Анализ омонимии в разделе результатов