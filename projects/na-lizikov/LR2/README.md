# Лабораторная работа №2: Векторизация текста

## Описание задания
В ходе лабораторной работы была выполнена векторная семантизация текста с использованием моделей Word2Vec. Работа включала следующие этапы:
- Предварительная обработка текстовых данных (токенизация, удаление стоп-слов)
- Обучение нескольких моделей Word2Vec с различными гиперпараметрами
- Сравнение качества полученных векторных представлений
- Векторизация целых документов через усреднение эмбеддингов слов
- Сохранение результатов в формате TSV для дальнейшего использования

## Использованные технологии и инструменты
- **Датасет**: AG News (новостные статьи, 120000 записей)
- **Модели**: Word2Vec (CBOW и Skip-gram архитектуры)
- **Библиотеки**:
  - Gensim 4.4.0
  - Scikit-learn
  - NLTK
  - NumPy
  - Pandas
  - Matplotlib
  - Scipy

## Результаты работы
1. **Обучение Word2Vec моделей**:
   - Размер словаря обученной модели: 22764 слов
   - Протестировано 5 различных комбинаций гиперпараметров:
     - Размер вектора: 50, 100, 200
     - Размер окна: 3, 5, 8, 10
     - Архитектура: CBOW (sg=0) и Skip-gram (sg=1)

2. **Качество векторных представлений**:
   - Наилучшие результаты показала модель с параметрами:
     - vector_size=100
     - window=5
     - sg=1 (Skip-gram)
   - Средние косинусные расстояния для тестовых слов:
     - Для похожих слов: 0.3594-0.5461
     - Для связанных слов: 0.2186-0.3826
     - Для разных слов: -0.1580-0.2665

3. **Примеры семантических связей**:
```
Слово: "apple"
Наиболее похожие: ['ipod', 'itunes', 'aapl', 'ipods', 'imac']

Слово: "microsoft"
Наиболее похожие: ['msft', 'windows', 'microsofts', 'xp', 'software']
```

4. **Визуализация**:
- Создана тепловая карта косинусных расстояний между векторными представлениями 20 документов
- Наглядно показаны семантические связи между различными текстами

## Выводы
1. **Архитектура модели**: Skip-gram показала лучшие результаты по сравнению с CBOW для данной задачи, что согласуется с теорией (Skip-gram лучше работает с редкими словами).

2. **Гиперпараметры**:
- Оптимальный размер вектора: 100 измерений
- Оптимальный размер окна: 5 слов
- Увеличение размера вектора до 200 не дало значительного улучшения качества

3. **Качество векторных представлений**:
- Модель успешно улавливает семантические связи между словами
- Близкие по смыслу слова имеют высокие косинусные сходства
- Слова из разных тематик имеют низкие или отрицательные сходства

## Инструкция по запуску

### Требования
Для запуска лабораторной работы необходим Python 3.8+ и следующие библиотеки:
```
pip install gensim==4.4.0
pip install scikit-learn
pip install nltk
pip install numpy
pip install pandas
pip install matplotlib
pip install scipy
```

### Шаги выполнения

1. **Установка зависимостей**:
```bash
pip install -r requirements.txt
```
2. **Загрузка датасета:**

Датсет [AG News](https://huggingface.co/datasets/wangrongsheng/ag_news/resolve/main/train.csv)

3. **Запуск лабораторной работы:**
```
jupyter notebook LR2.ipynb
```
Или выполните ноутбук в Google Colab, предварительно установив зависимости.

4. **Основные этапы выполнения:**

- Ячейка 1: Установка необходимых библиотек
- Ячейка 2: Импорт библиотек и загрузка NLTK данных
- Ячейка 3: Загрузка и предобработка датасета AG News
- Ячейка 4: Обучение базовой модели Word2Vec
- Ячейка 5: Эксперименты с гиперпараметрами Word2Vec
- Ячейка 6: Демонстрация работы лучшей модели
- Ячейка 7-9: Векторизация документов и сохранение результатов
- Ячейка 10: Визуализация результатов

5. **Результаты:**
- Векторные представления документов сохраняются в файл document_vectors.tsv
- Визуализация сохраняется как изображение в ноутбуке