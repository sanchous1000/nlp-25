# Лабораторная работа №1

## Описание задания

Целью лабораторной работы являлась реализация конвейера предварительной обработки текста на английском языке, включающего:

* сегментацию текста на предложения;
* токенизацию с корректной обработкой сложных лингвистических и формальных конструкций (например, адресов электронной почты, телефонных номеров, титулов и эмотиконов);
* стемминг с использованием алгоритма Snowball;
* лемматизацию с применением WordNetLemmatizer из библиотеки NLTK;
* формирование аннотированного корпуса в формате TSV, где для каждого токена указаны исходная форма, стемма и лемма.

В качестве исходных данных использовался английский Базовый новостной датасет, содержащий документы, размеченные по категориям. Для каждого документа был сформирован отдельный TSV-файл с трёхколоночной структурой:

`<токен> <стемма> <лемма>`

Особое внимание уделялось корректной обработке «сложных» случаев, таких как:

* адреса электронной почты (например, john.doe@example.com);
* телефонные номера (например, +1-555-123-4567, 555 987 6543);
* титулы с последующим именем (например, Dr. Smith, Prof. Johnson).

## Использованные технологии и инструменты
- Датасет: Базовый новостной датасет
- Модели: SnowballStemmer, WordNetLemmatizer
- Библиотеки: pandas, nltk

## Результаты работы

Реализованный конвейер успешно обработал все документы датасета. Сложные конструкции были корректно распознаны как единые токены и не разбиты на части. Например:

* `support@university.edu` — сохранён как один токен;
* `+1 (202) 555-0198` — не разбит по скобкам и дефисам;
* `Dr. Anderson` → обработан как единая лексическая единица.

* Для каждого документа был сгенерирован TSV-файл, размещённый в иерархической структуре каталогов в соответствии с меткой класса. Между предложениями вставляется пустая строка, что соответствует требованиям формата.

Пример фрагмента аннотации:

```
Dr. Anderson	Dr. Anderson	Dr. Anderson
contacted	contact	contact
us	us	u
at	at	at
help@service.com	help@service.com	help@service.com
```

Конвейер продемонстрировал устойчивость к разнообразным форматам входных данных и обеспечил высокую точность токенизации.

**Время работы:**

- Тренировочный набор: 7600 строк, 7 секунд
- Тестовый набор: 120000, 88 секунд

## Анализ омонимии

В ходе лемматизации были выявлены случаи омонимии — когда одна и та же словоформа может соответствовать разным леммам в зависимости от контекста и грамматической роли.

**Пример неправильной обработки: слово “leaves”**

* “The wind leaves the trees bare” → глагол → лемма: `leave`
* “Autumn leaves fall” → глагол → лемма: `leave`

Также слово (`saw`, `v`) лемматизируется в `saw`, хотя другие глаголы прошедшего времени, например, `led` он обработал нормально: `lead` 

## Выводы

Лабораторная работа позволила реализовать полноценный NLP-конвейер для сегментации, токенизации, стемминга и лемматизации английского текста

* Разработан токенизатор, учитывающий специфические форматы (email, телефоны, титулы);
* Интегрированы стемминг и контекстно-зависимая лемматизация с использованием POS-тегов;
* Сформирован структурированный аннотированный корпус в требуемом формате.

## Инструкция по запуску
1. Установить зависимости: `pip install -r requirements.txt`
2. Загрузить датасет: 
   1. [train.csv](https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv)
   2. [test.csv](https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv)
3. Запустить скрипт: `python source/main.py --dataset train` (или `--dataset test`)