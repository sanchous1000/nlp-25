{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd791661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "df = pd.read_csv('train.csv', header=None)\n",
    "texts = df.iloc[:, 2].dropna().astype(str).tolist()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.split()\n",
    "\n",
    "corpus = [preprocess(t) for t in texts]\n",
    "corpus = [t for t in corpus if t]\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f24e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sets = [\n",
    "        {\n",
    "        'base_token': 'football',\n",
    "        'similar': ['soccer', 'baseball', 'hockey'],\n",
    "        'same_domain': ['player', 'team', 'game'],\n",
    "        'different': ['painting', 'chemistry', 'universe']\n",
    "    },\n",
    "    {\n",
    "        'base_token': 'airplane',\n",
    "        'similar': ['aircraft', 'jet', 'plane'],\n",
    "        'same_domain': ['airport', 'flight', 'pilot'],\n",
    "        'different': ['mathematics', 'cat', 'fish']\n",
    "    },\n",
    "    {\n",
    "        'base_token': 'physics',\n",
    "        'similar': ['science', 'chemistry', 'biology'],\n",
    "        'same_domain': ['experiment', 'research', 'theory'],\n",
    "        'different': ['music', 'dance', 'song']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d07475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Базовый токен: 'football'\n",
      "Ранжированный список по косинусному расстоянию:\n",
      "  1. 'hockey' [похожий] -> 0.3408\n",
      "  2. 'soccer' [похожий] -> 0.3504\n",
      "  3. 'baseball' [похожий] -> 0.4138\n",
      "  4. 'team' [та же область] -> 0.5268\n",
      "  5. 'player' [та же область] -> 0.5351\n",
      "  6. 'game' [та же область] -> 0.5628\n",
      "  7. 'universe' [разные] -> 0.8251\n",
      "  8. 'chemistry' [разные] -> 0.8639\n",
      "  9. 'painting' [разные] -> 0.8842\n",
      "\n",
      "Базовый токен: 'airplane'\n",
      "Ранжированный список по косинусному расстоянию:\n",
      "  1. 'aircraft' [похожий] -> 0.3949\n",
      "  2. 'jet' [похожий] -> 0.4619\n",
      "  3. 'plane' [похожий] -> 0.4785\n",
      "  4. 'flight' [та же область] -> 0.6951\n",
      "  5. 'pilot' [та же область] -> 0.7306\n",
      "  6. 'airport' [та же область] -> 0.7383\n",
      "  7. 'fish' [разные] -> 0.7554\n",
      "  8. 'cat' [разные] -> 0.8957\n",
      "  9. 'mathematics' [разные] -> 0.9603\n",
      "\n",
      "Базовый токен: 'physics'\n",
      "Ранжированный список по косинусному расстоянию:\n",
      "  1. 'science' [похожий] -> 0.3290\n",
      "  2. 'chemistry' [похожий] -> 0.4235\n",
      "  3. 'biology' [похожий] -> 0.4628\n",
      "  4. 'research' [та же область] -> 0.6337\n",
      "  5. 'experiment' [та же область] -> 0.7070\n",
      "  6. 'theory' [та же область] -> 0.7557\n",
      "  7. 'dance' [разные] -> 0.7763\n",
      "  8. 'music' [разные] -> 0.9774\n",
      "  9. 'song' [разные] -> 1.0004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_distance(word1, word2):\n",
    "    if word1 in model.wv and word2 in model.wv:\n",
    "        return cosine(model.wv[word1], model.wv[word2])\n",
    "    return None\n",
    "\n",
    "for token_set in token_sets:\n",
    "    base = token_set['base_token']\n",
    "    \n",
    "    if base not in model.wv:\n",
    "        continue\n",
    "    \n",
    "    all_tokens = []\n",
    "    distances = []\n",
    "    \n",
    "    for token in token_set['similar']:\n",
    "        dist = get_distance(base, token)\n",
    "        if dist is not None:\n",
    "            all_tokens.append((token, 'похожий'))\n",
    "            distances.append(dist)\n",
    "    \n",
    "    for token in token_set['same_domain']:\n",
    "        dist = get_distance(base, token)\n",
    "        if dist is not None:\n",
    "            all_tokens.append((token, 'та же область'))\n",
    "            distances.append(dist)\n",
    "    \n",
    "    for token in token_set['different']:\n",
    "        dist = get_distance(base, token)\n",
    "        if dist is not None:\n",
    "            all_tokens.append((token, 'разные'))\n",
    "            distances.append(dist)\n",
    "    \n",
    "    sorted_items = sorted(zip(all_tokens, distances), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"Базовый токен: '{base}'\")\n",
    "    print(\"Ранжированный список по косинусному расстоянию:\")\n",
    "    for i, ((token, category), dist) in enumerate(sorted_items):\n",
    "        print(f\"  {i+1}. '{token}' [{category}] -> {dist:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d424cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def vectorize_document(text, vector_size=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = preprocess(sentence)\n",
    "        \n",
    "        if not tokens:\n",
    "            continue\n",
    "            \n",
    "        token_vectors = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                token_vectors.append(model.wv[token])\n",
    "        \n",
    "        if token_vectors:\n",
    "            sentence_vector = np.mean(token_vectors, axis=0)\n",
    "            sentence_vectors.append(sentence_vector)\n",
    "    \n",
    "    if sentence_vectors:\n",
    "        document_vector = np.mean(sentence_vectors, axis=0)\n",
    "        return document_vector\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81c56b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv', header=None)\n",
    "test_texts = df_test.iloc[:, 2].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e44fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train_data.tsv'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    for doc_id, text in enumerate(texts, 0):\n",
    "        vector = vectorize_document(text)\n",
    "        vector_str = \"\\t\".join([str(f\"{val:.6f}\") for val in vector])\n",
    "        line = f\"{doc_id+1}\\t{vector_str}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc5f9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_data.tsv'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    for doc_id, text in enumerate(test_texts, 0):\n",
    "        vector = vectorize_document(text)\n",
    "        vector_str = \"\\t\".join([str(f\"{val:.6f}\") for val in vector])\n",
    "        line = f\"{doc_id+1}\\t{vector_str}\\n\"\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
