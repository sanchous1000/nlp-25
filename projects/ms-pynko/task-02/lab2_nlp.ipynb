{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9de87b5-be7f-4343-af25-e29044cf953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (4.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from gensim) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: wrapt in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/maksim/Документы/jupiter/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4d3158-1b09-4070-8675-e10f073dc6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294e6418-595b-4139-8b2c-07034859a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_CORPUS_DIR = Path(\"projects/ms-pynko/assets/annotated-corpus\")\n",
    "\n",
    "def tokens_from_tsv(tsv_path: Path, col: int = 1, drop_punct: bool = False):\n",
    "    \"\"\"\n",
    "    Возвращает список токенов из tsv-файла.\n",
    "    col: 0/1/2 — какая колонка используется как токен.\n",
    "    drop_punct: если True, выкидывает токены, где нет букв/цифр.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    with tsv_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # иногда пустые строки\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) < 3:\n",
    "                # если вдруг разделено пробелами, подстрахуемся\n",
    "                parts = line.split()\n",
    "            if len(parts) <= col:\n",
    "                continue\n",
    "            tok = parts[col].strip()\n",
    "            if not tok:\n",
    "                continue\n",
    "            if drop_punct:\n",
    "                # оставляем токены, где есть буквы/цифры\n",
    "                if not any(ch.isalnum() for ch in tok):\n",
    "                    continue\n",
    "            tokens.append(tok)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83df9f53-74b0-4d7b-a1cb-6807b1a3841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True projects/ms-pynko/assets/annotated-corpus/train/1/000492.tsv\n",
      "['venezuelan', 'vote', 'earli', 'in', 'referendum', 'on', 'chavez', 'rule', '(', 'reuter', ')', '.', 'reuter', '-', 'venezuelan', 'turn', 'out', 'earli', '\\\\', 'and']\n"
     ]
    }
   ],
   "source": [
    "sample_file = BASE_CORPUS_DIR / \"train\" / \"1\" / \"000492.tsv\"\n",
    "print(sample_file.exists(), sample_file)\n",
    "print(tokens_from_tsv(sample_file, col=1)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5330fcbe-8931-4678-854c-51a31237df9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 7600, ['000492', '000493', '000494'], ['1', '1', '1'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_split_docs(split: str, col: int = 1, drop_punct: bool = False):\n",
    "    \"\"\"\n",
    "    split: 'train' или 'test'\n",
    "    Возвращает:\n",
    "      doc_ids: список идентификаторов документов (имена файлов без .tsv)\n",
    "      docs:    список списков токенов (по документу)\n",
    "      labels:  список тем (1..4) как строки\n",
    "    \"\"\"\n",
    "    split_dir = BASE_CORPUS_DIR / split\n",
    "    doc_ids, docs, labels = [], [], []\n",
    "    \n",
    "    for label_dir in sorted(split_dir.iterdir()):\n",
    "        if not label_dir.is_dir():\n",
    "            continue\n",
    "        label = label_dir.name  # '1','2','3','4'\n",
    "        for tsv_path in sorted(label_dir.glob(\"*.tsv\")):\n",
    "            toks = tokens_from_tsv(tsv_path, col=col, drop_punct=drop_punct)\n",
    "            if not toks:\n",
    "                continue\n",
    "            doc_ids.append(tsv_path.stem)\n",
    "            docs.append(toks)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return doc_ids, docs, labels\n",
    "\n",
    "train_ids, train_docs, train_labels = load_split_docs(\"train\", col=1, drop_punct=False)\n",
    "test_ids,  test_docs,  test_labels  = load_split_docs(\"test\",  col=1, drop_punct=False)\n",
    "\n",
    "len(train_docs), len(test_docs), train_ids[:3], train_labels[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a678f88-445e-4012-a42d-1371ba558cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_docs,   # <-- вместо regex/предложений\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc489e7-02d3-4f56-8468-1b4978d567fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def doc_to_vector(tokens, model: Word2Vec, dim: int):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(dim, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b637ff-b8a1-491d-8594-34278c83ce52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorize test: 100%|████████████████| 7600/7600 [00:00<00:00, 19088.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_embeddings_w2v.tsv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings = []\n",
    "doc_ids = []\n",
    "\n",
    "for doc_id, toks in tqdm(zip(test_ids, test_docs), total=len(test_docs), desc=\"Vectorize test\"):\n",
    "    embeddings.append(doc_to_vector(toks, w2v_model, EMBEDDING_DIM))\n",
    "    doc_ids.append(doc_id)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "result_df = pd.DataFrame(embeddings)\n",
    "result_df.insert(0, \"doc_id\", doc_ids)\n",
    "\n",
    "OUTPUT_PATH = \"test_embeddings_w2v.tsv\"\n",
    "result_df.to_csv(OUTPUT_PATH, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2ce301-e9dd-4aad-8f93-85c5b9f0587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   label                                              title  \\\n",
       " 0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       " 1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       " 2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       " 3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       " 4      3  Oil prices soar to all-time record, posing new...   \n",
       " \n",
       "                                                 text  \n",
       " 0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       " 1  Reuters - Private investment firm Carlyle Grou...  \n",
       " 2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       " 3  Reuters - Authorities have halted oil export\\f...  \n",
       " 4  AFP - Tearaway world oil prices, toppling reco...  ,\n",
       " Index(['label', 'title', 'text'], dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAIN_PATH = \"projects/ms-pynko/assets/dataset/news_train.csv\"\n",
    "TEST_PATH  = \"projects/ms-pynko/assets/dataset/news_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    TRAIN_PATH,\n",
    "    sep=\",\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"title\", \"text\"],   # сами задаём имена колонок\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    TEST_PATH,\n",
    "    sep=\",\",\n",
    "    header=None,\n",
    "    names=[\"title\", \"text\"],\n",
    ")\n",
    "\n",
    "train_df.head(), train_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a7bae5-502d-43c7-8b2e-8a182610f435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuters',\n",
       " 'short',\n",
       " 'sellers',\n",
       " 'wall',\n",
       " 'street',\n",
       " 's',\n",
       " 'dwindling',\n",
       " 'band',\n",
       " 'of',\n",
       " 'ultra',\n",
       " 'cynics',\n",
       " 'are',\n",
       " 'seeing',\n",
       " 'green',\n",
       " 'again']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "TEXT_COL = \"text\"   # колонка с текстом\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-zA-Zа-яА-ЯёЁ]+\", re.UNICODE)\n",
    "\n",
    "def text_to_tokens(text: str):\n",
    "    text = str(text).lower()\n",
    "    tokens = TOKEN_RE.findall(text)\n",
    "    return tokens\n",
    "\n",
    "# проверка\n",
    "example_text = train_df[TEXT_COL].iloc[0]\n",
    "text_to_tokens(example_text)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c0a5d52-a3b1-40e4-80b0-ca960ac8eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Подготовка предложений: 100%|███████| 120000/120000 [00:00<00:00, 158673.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214647,\n",
       " ['reuters',\n",
       "  'short',\n",
       "  'sellers',\n",
       "  'wall',\n",
       "  'street',\n",
       "  's',\n",
       "  'dwindling',\n",
       "  'band',\n",
       "  'of',\n",
       "  'ultra'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_to_sentences(text: str):\n",
    "    # грубое разбиение по .?!…\n",
    "    sentences = re.split(r\"[.!?…]+\", str(text))\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "sentences_tokens = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for text in tqdm(train_df[TEXT_COL], desc=\"Подготовка предложений\"):\n",
    "    for sent in split_to_sentences(text):\n",
    "        tokens = text_to_tokens(sent)\n",
    "        if tokens:\n",
    "            sentences_tokens.append(tokens)\n",
    "\n",
    "len(sentences_tokens), sentences_tokens[0][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39a97d6b-0202-48cf-a14b-bf7ea5819c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', 'a', 'to', 'of', 'in', 'and', 's', 'on', 'for', 'that']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences_tokens,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "list(w2v_model.wv.key_to_index.keys())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c266346b-29aa-4798-9e09-9ea1615b1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word: str, model: Word2Vec, dim: int):\n",
    "    if word in model.wv:\n",
    "        return model.wv[word]\n",
    "    else:\n",
    "        return np.zeros(dim, dtype=\"float32\")\n",
    "\n",
    "def cosine_distance(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    num = np.dot(vec1, vec2)\n",
    "    den = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    if den == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - num / den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7dd83b9-98c7-45a8-9b80-9a516c29ded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Якорное слово: oil\n",
      "\n",
      "Похожие по смыслу:\n",
      "  crude                dist = 0.1553\n",
      "  price                dist = 0.5437\n",
      "\n",
      "Из той же предметной области:\n",
      "  stock                dist = 0.5449\n",
      "  market               dist = 0.6982\n",
      "\n",
      "Далёкие по смыслу:\n",
      "  cat                  dist = 0.8120\n",
      "  phone                dist = 0.7263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_word_groups(anchor, similar_words, same_domain_words, different_words, model, dim):\n",
    "    def dist(w):\n",
    "        v1 = get_word_vector(anchor, model, dim)\n",
    "        v2 = get_word_vector(w, model, dim)\n",
    "        return cosine_distance(v1, v2)\n",
    "    \n",
    "    print(f\"Якорное слово: {anchor}\\n\")\n",
    "    for name, group in [\n",
    "        (\"Похожие по смыслу\", similar_words),\n",
    "        (\"Из той же предметной области\", same_domain_words),\n",
    "        (\"Далёкие по смыслу\", different_words),\n",
    "    ]:\n",
    "        print(name + \":\")\n",
    "        for w in group:\n",
    "            print(f\"  {w:<20} dist = {dist(w):.4f}\")\n",
    "        print()\n",
    "\n",
    "demo_word_groups(\n",
    "    anchor=\"oil\",                     # пример, поменяй при желании\n",
    "    similar_words=[\"crude\", \"price\"],\n",
    "    same_domain_words=[\"stock\", \"market\"],\n",
    "    different_words=[\"cat\", \"phone\"],\n",
    "    model=w2v_model,\n",
    "    dim=EMBEDDING_DIM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0304902-aaca-450d-9421-8fa3a159d973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,),\n",
       " array([ 0.02629399,  0.00785776, -0.06658947,  0.183035  , -0.24346587,\n",
       "        -0.27846712,  0.06121453,  0.25208753, -0.17313185, -0.20860425],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_vector(sentence: str, model: Word2Vec, dim: int):\n",
    "    tokens = text_to_tokens(sentence)\n",
    "    if not tokens:\n",
    "        return np.zeros(dim, dtype=\"float32\")\n",
    "    vecs = [get_word_vector(t, model, dim) for t in tokens]\n",
    "    if not vecs:\n",
    "        return np.zeros(dim, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def text_to_document_vector(text: str, model: Word2Vec, dim: int):\n",
    "    sentences = split_to_sentences(text)\n",
    "    sent_vecs = []\n",
    "    for sent in sentences:\n",
    "        v = sentence_to_vector(sent, model, dim)\n",
    "        if np.linalg.norm(v) > 0:\n",
    "            sent_vecs.append(v)\n",
    "    if not sent_vecs:\n",
    "        return np.zeros(dim, dtype=\"float32\")\n",
    "    return np.mean(sent_vecs, axis=0)\n",
    "\n",
    "# проверка\n",
    "doc_vec = text_to_document_vector(train_df[TEXT_COL].iloc[0], w2v_model, EMBEDDING_DIM)\n",
    "doc_vec.shape, doc_vec[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a17f448-73b1-4620-8e4e-39f7f21bd727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация теста: 100%|████████████████| 7600/7600 [00:00<00:00, 19472.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_embeddings_w2v.tsv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "doc_ids = []\n",
    "\n",
    "for idx, text in tqdm(enumerate(test_df[\"text\"]), total=len(test_df), desc=\"Векторизация теста\"):\n",
    "    vec = text_to_document_vector(text, w2v_model, EMBEDDING_DIM)\n",
    "    embeddings.append(vec)\n",
    "    doc_ids.append(str(idx))   # или f\"{idx:03d}\", если нужны нули спереди\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "result_df = pd.DataFrame(embeddings)\n",
    "result_df.insert(0, \"doc_id\", doc_ids)\n",
    "\n",
    "OUTPUT_PATH = \"test_embeddings_w2v.tsv\"\n",
    "result_df.to_csv(OUTPUT_PATH, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a2cac-3772-4d93-abdd-1e043d1e8fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
