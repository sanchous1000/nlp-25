{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6c5749-5193-4f93-91da-dc46017c40d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (2.3.3)\n",
      "Requirement already satisfied: nltk in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (3.9.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: joblib in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: click in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/maksim/Документы/jupiter/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a5c705-9934-4d23-8c8b-312aae66ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/maksim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/maksim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # словари для разных языков wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32348d29-9607-4f36-a9d4-c2756c810159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (2.32.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from requests) (2.6.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages (from requests) (3.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/maksim/Документы/jupiter/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b7a2a3-c8da-4f68-a936-6fe40c307b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV: /Users/maksim/Документы/jupiter/nlp/projects/ms-pynko/assets/dataset/news_train.csv\n",
      "Test  CSV: /Users/maksim/Документы/jupiter/nlp/projects/ms-pynko/assets/dataset/news_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim/Документы/jupiter/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Лабораторная работа 1\n",
    "# Сегментация, токенизация, стемминг, лемматизация, аннотация корпуса\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "PROJECT_NAME = \"ms-pynko\"  \n",
    "\n",
    "# Пути\n",
    "repo_root = Path(\".\").resolve()\n",
    "dataset_dir = repo_root / \"projects\" / PROJECT_NAME / \"assets\" / \"dataset\"\n",
    "annotated_dir = repo_root / \"projects\" / PROJECT_NAME / \"assets\" / \"annotated-corpus\"\n",
    "\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "annotated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n",
    "TEST_URL  = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\"\n",
    "\n",
    "train_csv_path = dataset_dir / \"news_train.csv\"\n",
    "test_csv_path  = dataset_dir / \"news_test.csv\"\n",
    "\n",
    "print(\"Train CSV:\", train_csv_path)\n",
    "print(\"Test  CSV:\", test_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90eaa8d-7d3e-4799-aa50-5be40282be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, dest_path):\n",
    "    print(f\"Скачиваю {url} -> {dest_path}\")\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    with open(dest_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Готово!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cffcd2-0eca-421c-b09c-22a8c8f36c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачиваю https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv -> /Users/maksim/Документы/jupiter/nlp/projects/ms-pynko/assets/dataset/news_train.csv\n",
      "Готово!\n",
      "Скачиваю https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv -> /Users/maksim/Документы/jupiter/nlp/projects/ms-pynko/assets/dataset/news_test.csv\n",
      "Готово!\n",
      "Train shape: (120000, 3)\n",
      "Test shape: (7600, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(TRAIN_URL, train_csv_path)\n",
    "download(TEST_URL,  test_csv_path)\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path, header=None, names=[\"label\", \"title\", \"text\"])\n",
    "test_df  = pd.read_csv(test_csv_path, header=None, names=[\"label\", \"title\", \"text\"])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d389a9b-bf39-4f9e-b9fa-eeca64703b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc_id label                                               text\n",
      "0  000000     3  Wall St. Bears Claw Back Into the Black (Reute...\n",
      "1  000001     3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
      "2  000002     3  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
      "3  000003     3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
      "4  000004     3  Oil prices soar to all-time record, posing new...\n",
      "Train std shape: (120000, 3)\n",
      "Test std shape: (7600, 3)\n"
     ]
    }
   ],
   "source": [
    "# создаём doc_id как zero-padded индекс: 000001, 000002, ...\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df[\"doc_id\"] = train_df.index.map(lambda i: f\"{i:06d}\")\n",
    "test_df[\"doc_id\"]  = test_df.index.map(lambda i: f\"{i:06d}\")\n",
    "\n",
    "# label будем хранить как строку (1,2,3,4) — они станут именами папок\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
    "test_df[\"label\"]  = test_df[\"label\"].astype(str)\n",
    "\n",
    "# текст = заголовок + описание\n",
    "train_df[\"text_full\"] = (\n",
    "    train_df[\"title\"].fillna(\"\") + \". \" + train_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "test_df[\"text_full\"] = (\n",
    "    test_df[\"title\"].fillna(\"\") + \". \" + test_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "# Оставим только нужные три колонки в отдельные датафреймы\n",
    "train_std = train_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "test_std  = test_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "\n",
    "print(train_std.head())\n",
    "print(\"Train std shape:\", train_std.shape)\n",
    "print(\"Test std shape:\",  test_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a2ee95-91a0-4d1f-ad47-4d4057d2a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK инициализировано\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"NLTK инициализировано\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efc0efe4-0ab7-43d7-a92e-79d43a89cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dr. Smith spoke to Ms. Brown.\n",
      "→ Mrs. Johnson agreed!\n",
      "→ This works.\n"
     ]
    }
   ],
   "source": [
    "ABBREVIATIONS = [\n",
    "    \"Mr\", \"Ms\", \"Mrs\", \"Dr\", \"Prof\", \"Inc\", \"Ltd\", \"Jr\", \"Sr\",\n",
    "    \"U.S\", \"U.K\", \"St\", \"Univ\"\n",
    "]\n",
    "def protect_abbreviations(text: str):\n",
    "    for abbr in ABBREVIATIONS:\n",
    "        text = re.sub(\n",
    "            rf\"\\b{abbr}\\.\",\n",
    "            abbr.replace(\".\", \"<DOT>\") + \"<DOT>\",\n",
    "            text\n",
    "        )\n",
    "    return text\n",
    "sentence_end_re = re.compile(r'(?<=[.!?])\\s+')\n",
    "def restore_abbreviations(text: str):\n",
    "    return text.replace(\"<DOT>\", \".\")\n",
    "def split_to_sentences(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # защищаем сокращения\n",
    "    protected = protect_abbreviations(text)\n",
    "\n",
    "    # делим на предложения\n",
    "    parts = sentence_end_re.split(protected)\n",
    "\n",
    "    # возвращаем точки\n",
    "    parts = [restore_abbreviations(p) for p in parts]\n",
    "\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "test_text = \"Dr. Smith spoke to Ms. Brown. Mrs. Johnson agreed! This works.\"\n",
    "\n",
    "for s in split_to_sentences(test_text):\n",
    "    print(\"→\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a439cf5-e069-4625-ad00-165aafc12111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Email', 'me', 'at', 'test@example.com', 'or', 'call', '+1-202-555-01-23', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# токенизатор с \"усложнёнными\" случаями\n",
    "token_pattern = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:\\+?\\d[\\d\\-\\(\\)\\s]{7,}\\d)                 # телефоны, напр. +7-901-000-00-00\n",
    "    | (?:[\\w\\.-]+@[\\w\\.-]+\\.\\w+)                # email, напр. abc@xyz.com\n",
    "    | (?:[:;=8][\\-^']?[)DdpP(/\\\\])              # смайлики :) ;-) :D\n",
    "    | (?:\\d+(?:[.,]\\d+)*)                       # числа\n",
    "    | (?:[A-Za-zА-Яа-яЁё]+(?:[-'][A-Za-zА-Яа-яЁё]+)*)  # слова с дефисами/апострофами\n",
    "    | (?:[^\\s])                                 # одиночный не-пробельный символ\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return [m.group(0) for m in token_pattern.finditer(text)]\n",
    "\n",
    "# мини-тест\n",
    "print(tokenize(\"Email me at test@example.com or call +1-202-555-01-23 :)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dcf1428-53a9-4033-908a-371ede9b8215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs -> dog / dog\n",
      "were -> were / were\n",
      "running -> run / running\n",
      ", -> , / ,\n",
      "cats -> cat / cat\n",
      "slept -> slept / slept\n",
      ". -> . / .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_token(token: str) -> str:\n",
    "    if re.fullmatch(r\"[A-Za-z]+\", token):\n",
    "        return stemmer.stem(token.lower())\n",
    "    return token.lower()\n",
    "\n",
    "def lemmatize_token(token: str, pos: str = \"n\") -> str:\n",
    "    if re.fullmatch(r\"[A-Za-z]+\", token):\n",
    "        return lemmatizer.lemmatize(token.lower(), pos=pos)\n",
    "    return token.lower()\n",
    "\n",
    "# мини-тест\n",
    "for t in tokenize(\"Dogs were running, cats slept.\"):\n",
    "    print(t, \"->\", stem_token(t), \"/\", lemmatize_token(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86be1441-4c4d-4bcb-9ecd-36575830a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dogs', 'dog', 'dog'), ('were', 'were', 'were'), ('running', 'run', 'running'), ('fast', 'fast', 'fast'), ('!', '!', '!')]\n"
     ]
    }
   ],
   "source": [
    "def annotate_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Одно предложение -> список троек (token, stem, lemma)\n",
    "    \"\"\"\n",
    "    tokens = tokenize(sentence)\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        stem = stem_token(tok)\n",
    "        lemma = lemmatize_token(tok)\n",
    "        out.append((tok, stem, lemma))\n",
    "    return out\n",
    "\n",
    "# тест\n",
    "print(annotate_sentence(\"Dogs were running fast!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd317f6-121e-4685-8278-632090d5b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_document(text: str):\n",
    "    \"\"\"\n",
    "    Текст -> список предложений, каждое предложение -> список троек (token, stem, lemma)\n",
    "    \"\"\"\n",
    "    sentences = split_to_sentences(text)\n",
    "    return [annotate_sentence(s) for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d9c1d3-6a13-42bd-8bf7-0485aeb74de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinic\tclinic\tclinic\n",
      "is\tis\tis\n",
      "located\tlocat\tlocated\n",
      "in\tin\tin\n",
      "the\tthe\tthe\n",
      "center\tcenter\tcenter\n",
      "of\tof\tof\n",
      "the\tthe\tthe\n",
      "city\tciti\tcity\n",
      ".\t.\t.\n",
      "\n",
      "It\tit\tit\n",
      "opened\topen\topened\n",
      "in\tin\tin\n",
      "1995\t1995\t1995\n",
      ".\t.\t.\n"
     ]
    }
   ],
   "source": [
    "def doc_to_tsv_str(text: str) -> str:\n",
    "    \"\"\"\n",
    "    <token>\\t<stem>\\t<lemma>\n",
    "    пустая строка между предложениями\n",
    "    \"\"\"\n",
    "    sentences_ann = annotate_document(text)\n",
    "    lines = []\n",
    "    for sent_ann in sentences_ann:\n",
    "        for token, stem, lemma in sent_ann:\n",
    "            lines.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "        lines.append(\"\")  # пустая строка между предложениями\n",
    "\n",
    "    # убрать последнюю пустую строку если есть\n",
    "    if lines and lines[-1] == \"\":\n",
    "        lines = lines[:-1]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# тест\n",
    "sample = \"Clinic is located in the center of the city. It opened in 1995.\"\n",
    "print(doc_to_tsv_str(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f61407a-3fad-4189-8c38-5b34d3033843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000</td>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002</td>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000003</td>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000004</td>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id label                                               text\n",
       "0  000000     3  Wall St. Bears Claw Back Into the Black (Reute...\n",
       "1  000001     3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  000002     3  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
       "3  000003     3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  000004     3  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_csv_path, header=None, names=[\"label\", \"title\", \"text\"])\n",
    "test_df  = pd.read_csv(test_csv_path,  header=None, names=[\"label\", \"title\", \"text\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df[\"doc_id\"] = train_df.index.map(lambda i: f\"{i:06d}\")\n",
    "test_df[\"doc_id\"]  = test_df.index.map(lambda i: f\"{i:06d}\")\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
    "test_df[\"label\"]  = test_df[\"label\"].astype(str)\n",
    "\n",
    "train_df[\"text_full\"] = (\n",
    "    train_df[\"title\"].fillna(\"\") + \". \" + train_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "test_df[\"text_full\"] = (\n",
    "    test_df[\"title\"].fillna(\"\") + \". \" + test_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "train_std = train_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "test_std  = test_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "\n",
    "train_std.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44be368b-963e-4388-bb16-22cb41542163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "annotated_dir = Path(f\"projects/{PROJECT_NAME}/assets/annotated-corpus\")\n",
    "annotated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_annotated_subset_from_df(df, subset_name: str):\n",
    "    subset_root = annotated_dir / subset_name\n",
    "    subset_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = str(row[\"doc_id\"])\n",
    "        label = str(row[\"label\"])\n",
    "        text  = str(row[\"text\"])\n",
    "\n",
    "        label_dir = subset_root / label\n",
    "        label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        tsv_path = label_dir / f\"{doc_id}.tsv\"\n",
    "        tsv_content = doc_to_tsv_str(text)\n",
    "\n",
    "        with open(tsv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tsv_content)\n",
    "\n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"{subset_name}: обработано {idx} документов...\")\n",
    "\n",
    "    print(f\"Готово: {subset_name} → {subset_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3002a054-6482-4350-8561-1f45d33ca0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: обработано 0 документов...\n",
      "train: обработано 5000 документов...\n",
      "train: обработано 10000 документов...\n",
      "train: обработано 15000 документов...\n",
      "train: обработано 20000 документов...\n",
      "train: обработано 25000 документов...\n",
      "train: обработано 30000 документов...\n",
      "train: обработано 35000 документов...\n",
      "train: обработано 40000 документов...\n",
      "train: обработано 45000 документов...\n",
      "train: обработано 50000 документов...\n",
      "train: обработано 55000 документов...\n",
      "train: обработано 60000 документов...\n",
      "train: обработано 65000 документов...\n",
      "train: обработано 70000 документов...\n",
      "train: обработано 75000 документов...\n",
      "train: обработано 80000 документов...\n",
      "train: обработано 85000 документов...\n",
      "train: обработано 90000 документов...\n",
      "train: обработано 95000 документов...\n",
      "train: обработано 100000 документов...\n",
      "train: обработано 105000 документов...\n",
      "train: обработано 110000 документов...\n",
      "train: обработано 115000 документов...\n",
      "Готово: train → projects/ms-pynko/assets/annotated-corpus/train\n",
      "test: обработано 0 документов...\n",
      "test: обработано 5000 документов...\n",
      "Готово: test → projects/ms-pynko/assets/annotated-corpus/test\n"
     ]
    }
   ],
   "source": [
    "create_annotated_subset_from_df(train_std, \"train\")\n",
    "create_annotated_subset_from_df(test_std, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88484cd6-ad1a-4691-ac00-5cbb194db3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл: projects/ms-pynko/assets/annotated-corpus/train/1/009763.tsv\n",
      "--- Содержимое фрагмента ---\n",
      "New\tnew\tnew\n",
      "Polio\tpolio\tpolio\n",
      "Cases\tcase\tcase\n",
      "Reported\treport\treported\n",
      "in\tin\tin\n",
      "Sudan\tsudan\tsudan\n",
      ".\t.\t.\n",
      "\n",
      "Global\tglobal\tglobal\n",
      "health\thealth\thealth\n",
      "officials\toffici\tofficial\n",
      "say\tsay\tsay\n",
      "war-torn\twar-torn\twar-torn\n",
      "Sudan\tsudan\tsudan\n",
      "is\tis\tis\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "sample_label = random.choice([\"1\", \"2\", \"3\", \"4\"])\n",
    "sample_file = next((annotated_dir / \"train\" / sample_label).glob(\"*.tsv\"))\n",
    "\n",
    "print(\"Файл:\", sample_file)\n",
    "print(\"--- Содержимое фрагмента ---\")\n",
    "with open(sample_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(15):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "275ea46b-530d-4161-8d72-4ff390398e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left     → n: left     | v: leave   \n",
      "better   → a: good     | v: better  \n",
      "meeting  → n: meeting  | v: meet    \n",
      "building → n: building | v: build   \n",
      "written  → a: written  | v: write   \n",
      "lost     → a: lost     | v: lose    \n",
      "bound    → a: bound    | v: bind    \n",
      "closed   → a: closed   | v: close   \n",
      "increased → a: increased | v: increase\n",
      "reduced  → a: reduced  | v: reduce  \n",
      "produced → a: produced | v: produce \n",
      "developed → a: developed | v: develop \n",
      "broken   → a: broken   | v: break   \n",
      "grown    → a: grown    | v: grow    \n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    (\"left\", \"n\", \"v\"),        # left / leave\n",
    "    (\"better\", \"a\", \"v\"),      # good / better\n",
    "    (\"meeting\", \"n\", \"v\"),     # meeting / meet\n",
    "    (\"building\", \"n\", \"v\"),    # building / build\n",
    "    (\"written\", \"a\", \"v\"),     # written / write\n",
    "    (\"lost\", \"a\", \"v\"),        # lost / lose\n",
    "    (\"bound\", \"a\", \"v\"),       # bound / bind\n",
    "    (\"closed\", \"a\", \"v\"),      # closed / close\n",
    "    (\"increased\", \"a\", \"v\"),   # increased / increase\n",
    "    (\"reduced\", \"a\", \"v\"),     # reduced / reduce\n",
    "    (\"produced\", \"a\", \"v\"),    # produced / produce\n",
    "    (\"developed\", \"a\", \"v\"),   # developed / develop\n",
    "    (\"broken\", \"a\", \"v\"),      # broken / break\n",
    "    (\"grown\", \"a\", \"v\"),       # grown / grow\n",
    "]\n",
    "\n",
    "\n",
    "for word, pos1, pos2 in examples:\n",
    "    lemma_1 = lemmatizer.lemmatize(word, pos=pos1)\n",
    "    lemma_2 = lemmatizer.lemmatize(word, pos=pos2)\n",
    "    print(f\"{word:8} → {pos1}: {lemma_1:8} | {pos2}: {lemma_2:8}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
