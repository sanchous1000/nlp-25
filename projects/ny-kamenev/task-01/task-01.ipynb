{
 "cells": [
  {
   "cell_type": "code",
   "id": "cb6c5749-5193-4f93-91da-dc46017c40d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:09.104573Z",
     "start_time": "2025-12-18T19:47:58.657863Z"
    }
   },
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "30a5c705-9934-4d23-8c8b-312aae66ce86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:21.005350Z",
     "start_time": "2025-12-18T19:48:09.109114Z"
    }
   },
   "source": [
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Загрузка punkt_tab...\")\n",
    "    nltk.download('punkt_tab', quiet=False)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Загрузка punkt...\")\n",
    "    nltk.download('punkt', quiet=False)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"Загрузка wordnet...\")\n",
    "    nltk.download('wordnet', quiet=False)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Загрузка stopwords...\")\n",
    "    nltk.download('stopwords', quiet=False)\n",
    "\n",
    "print(\"Библиотеки успешно импортированы и ресурсы NLTK загружены\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка wordnet...\n",
      "Библиотеки успешно импортированы и ресурсы NLTK загружены\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f2b7a2a3-c8da-4f68-a936-6fe40c307b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:21.018554Z",
     "start_time": "2025-12-18T19:48:21.010255Z"
    }
   },
   "source": [
    "# Загрузка датасета AG News\n",
    "train_url = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\"\n",
    "\n",
    "# Создание директории для данных\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Скачивание файлов\n",
    "if not (data_dir / \"train.csv\").exists():\n",
    "    print(\"Скачивание train.csv...\")\n",
    "    urllib.request.urlretrieve(train_url, data_dir / \"train.csv\")\n",
    "    print(\"train.csv загружен\")\n",
    "\n",
    "if not (data_dir / \"test.csv\").exists():\n",
    "    print(\"Скачивание test.csv...\")\n",
    "    urllib.request.urlretrieve(test_url, data_dir / \"test.csv\")\n",
    "    print(\"test.csv загружен\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "ea968501f389144d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:21.030667Z",
     "start_time": "2025-12-18T19:48:21.025109Z"
    }
   },
   "source": [
    "train_csv_path = data_dir / \"train.csv\"\n",
    "test_csv_path = data_dir / \"test.csv\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "32cffcd2-0eca-421c-b09c-22a8c8f36c3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:22.403023Z",
     "start_time": "2025-12-18T19:48:21.034139Z"
    }
   },
   "source": [
    "train_df = pd.read_csv(train_csv_path, header=None, names=[\"label\", \"title\", \"text\"])\n",
    "test_df  = pd.read_csv(test_csv_path, header=None, names=[\"label\", \"title\", \"text\"])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "train_df.head()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (120000, 3)\n",
      "Test shape: (7600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "5d389a9b-bf39-4f9e-b9fa-eeca64703b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:22.757468Z",
     "start_time": "2025-12-18T19:48:22.404563Z"
    }
   },
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df[\"doc_id\"] = train_df.index.map(lambda i: f\"{i:06d}\")\n",
    "test_df[\"doc_id\"]  = test_df.index.map(lambda i: f\"{i:06d}\")\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
    "test_df[\"label\"]  = test_df[\"label\"].astype(str)\n",
    "\n",
    "train_df[\"text_full\"] = (\n",
    "    train_df[\"title\"].fillna(\"\") + \". \" + train_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "test_df[\"text_full\"] = (\n",
    "    test_df[\"title\"].fillna(\"\") + \". \" + test_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "train_std = train_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "test_std  = test_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "\n",
    "print(train_std.head())\n",
    "print(\"Train std shape:\", train_std.shape)\n",
    "print(\"Test std shape:\",  test_std.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc_id label                                               text\n",
      "0  000000     3  Wall St. Bears Claw Back Into the Black (Reute...\n",
      "1  000001     3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
      "2  000002     3  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
      "3  000003     3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
      "4  000004     3  Oil prices soar to all-time record, posing new...\n",
      "Train std shape: (120000, 3)\n",
      "Test std shape: (7600, 3)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b3a2ee95-91a0-4d1f-ad47-4d4057d2a9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:48:22.766982Z",
     "start_time": "2025-12-18T19:48:22.760316Z"
    }
   },
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:49:59.674972Z",
     "start_time": "2025-12-18T19:49:59.663282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ABBREVIATIONS = [\n",
    "    \"Mr\", \"Ms\", \"Mrs\", \"Dr\", \"Prof\", \"Inc\", \"Ltd\", \"Jr\", \"Sr\",\n",
    "    \"U.S\", \"U.K\", \"St\", \"Univ\"\n",
    "]\n",
    "def protect_abbreviations(text: str):\n",
    "    for abbr in ABBREVIATIONS:\n",
    "        text = re.sub(\n",
    "            rf\"\\b{abbr}\\.\",\n",
    "            abbr.replace(\".\", \"<DOT>\") + \"<DOT>\",\n",
    "            text\n",
    "        )\n",
    "    return text\n",
    "\n",
    "sentence_end_re = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def restore_abbreviations(text: str):\n",
    "    return text.replace(\"<DOT>\", \".\")\n",
    "\n",
    "def split_to_sentences(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    protected = protect_abbreviations(text)\n",
    "\n",
    "    parts = sentence_end_re.split(protected)\n",
    "\n",
    "    parts = [restore_abbreviations(p) for p in parts]\n",
    "\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "test_text = \"Dr. Smith spoke to Ms. Brown. Mrs. Johnson agreed! This works.\"\n",
    "\n",
    "for s in split_to_sentences(test_text):\n",
    "    print(\"→\", s)"
   ],
   "id": "aeb07e6aded6b72a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dr. Smith spoke to Ms. Brown.\n",
      "→ Mrs. Johnson agreed!\n",
      "→ This works.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6a439cf5-e069-4625-ad00-165aafc12111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:50:47.938879Z",
     "start_time": "2025-12-18T19:50:47.930081Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "token_pattern = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:\\+?\\d[\\d\\-\\(\\)\\s]{7,}\\d)                 # телефоны, напр. +7-901-000-00-00\n",
    "    | (?:[\\w\\.-]+@[\\w\\.-]+\\.\\w+)                # email, напр. abc@xyz.com\n",
    "    | (?:[:;=8][\\-^']?[)DdpP(/\\\\])              # смайлики :) ;-) :D\n",
    "    | (?:\\d+(?:[.,]\\d+)*)                       # числа\n",
    "    | (?:[A-Za-zА-Яа-яЁё]+(?:[-'][A-Za-zА-Яа-яЁё]+)*)  # слова с дефисами/апострофами\n",
    "    | (?:[^\\s])                                 # одиночный не-пробельный символ\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return [m.group(0) for m in token_pattern.finditer(text)]\n",
    "\n",
    "tokens = tokenize(\"Email me at test@example.com or call +1-202-555-01-23\")\n",
    "print(\"Все токены:\", tokens)\n",
    "print(\"\\nСпециальные токены:\")\n",
    "for token in tokens:\n",
    "    if '@' in token:\n",
    "        print(f\"  {token} (EMAIL)\")\n",
    "    elif re.match(r'\\+?\\d[\\d\\-\\(\\)\\s]{7,}\\d', token):\n",
    "        print(f\"  {token} (PHONE)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все токены: ['Email', 'me', 'at', 'test@example.com', 'or', 'call', '+1-202-555-01-23']\n",
      "\n",
      "Специальные токены:\n",
      "  test@example.com (EMAIL)\n",
      "  +1-202-555-01-23 (PHONE)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2dcf1428-53a9-4033-908a-371ede9b8215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:50:52.902818Z",
     "start_time": "2025-12-18T19:50:49.873180Z"
    }
   },
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_token(token: str) -> str:\n",
    "    if re.fullmatch(r\"[A-Za-z]+\", token):\n",
    "        return stemmer.stem(token.lower())\n",
    "    return token.lower()\n",
    "\n",
    "def lemmatize_token(token: str, pos: str = \"n\") -> str:\n",
    "    if re.fullmatch(r\"[A-Za-z]+\", token):\n",
    "        return lemmatizer.lemmatize(token.lower(), pos=pos)\n",
    "    return token.lower()\n",
    "\n",
    "# мини-тест\n",
    "for t in tokenize(\"Dogs were running, cats slept.\"):\n",
    "    print(t, \"->\", stem_token(t), \"/\", lemmatize_token(t))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs -> dog / dog\n",
      "were -> were / were\n",
      "running -> run / running\n",
      ", -> , / ,\n",
      "cats -> cat / cat\n",
      "slept -> slept / slept\n",
      ". -> . / .\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "86be1441-4c4d-4bcb-9ecd-36575830a0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:50:55.543884Z",
     "start_time": "2025-12-18T19:50:55.537963Z"
    }
   },
   "source": [
    "def annotate_sentence(sentence: str):\n",
    "\n",
    "    tokens = tokenize(sentence)\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        stem = stem_token(tok)\n",
    "        lemma = lemmatize_token(tok)\n",
    "        out.append((tok, stem, lemma))\n",
    "    return out\n",
    "\n",
    "print(annotate_sentence(\"Dogs were running fast!\"))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dogs', 'dog', 'dog'), ('were', 'were', 'were'), ('running', 'run', 'running'), ('fast', 'fast', 'fast'), ('!', '!', '!')]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "b6d9c1d3-6a13-42bd-8bf7-0485aeb74de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:51:02.140554Z",
     "start_time": "2025-12-18T19:51:02.135008Z"
    }
   },
   "source": [
    "def annotate_document(text: str):\n",
    "    sentences = split_to_sentences(text)\n",
    "    return [annotate_sentence(s) for s in sentences]\n",
    "\n",
    "def doc_to_tsv_str(text: str) -> str:\n",
    "    \"\"\"\n",
    "    <token>\\t<stem>\\t<lemma>\n",
    "    \"\"\"\n",
    "    sentences_ann = annotate_document(text)\n",
    "    lines = []\n",
    "    for sent_ann in sentences_ann:\n",
    "        for token, stem, lemma in sent_ann:\n",
    "            lines.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    if lines and lines[-1] == \"\":\n",
    "        lines = lines[:-1]\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8f61407a-3fad-4189-8c38-5b34d3033843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:51:04.579792Z",
     "start_time": "2025-12-18T19:51:03.680363Z"
    }
   },
   "source": [
    "train_df = pd.read_csv(train_csv_path, header=None, names=[\"label\", \"title\", \"text\"])\n",
    "test_df  = pd.read_csv(test_csv_path,  header=None, names=[\"label\", \"title\", \"text\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df[\"doc_id\"] = train_df.index.map(lambda i: f\"{i:06d}\")\n",
    "test_df[\"doc_id\"]  = test_df.index.map(lambda i: f\"{i:06d}\")\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
    "test_df[\"label\"]  = test_df[\"label\"].astype(str)\n",
    "\n",
    "train_df[\"text_full\"] = (\n",
    "    train_df[\"title\"].fillna(\"\") + \". \" + train_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "test_df[\"text_full\"] = (\n",
    "    test_df[\"title\"].fillna(\"\") + \". \" + test_df[\"text\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "train_std = train_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "test_std  = test_df[[\"doc_id\", \"label\", \"text_full\"]].rename(columns={\"text_full\": \"text\"})\n",
    "\n",
    "train_std.head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   doc_id label                                               text\n",
       "0  000000     3  Wall St. Bears Claw Back Into the Black (Reute...\n",
       "1  000001     3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  000002     3  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
       "3  000003     3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  000004     3  Oil prices soar to all-time record, posing new..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000</td>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002</td>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000003</td>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000004</td>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "44be368b-963e-4388-bb16-22cb41542163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:51:56.531453Z",
     "start_time": "2025-12-18T19:51:56.522011Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "annotated_dir = Path(f\"assets/annotated-corpus\")\n",
    "annotated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_annotated_subset_from_df(df, subset_name: str):\n",
    "    subset_root = annotated_dir / subset_name\n",
    "    subset_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = str(row[\"doc_id\"])\n",
    "        label = str(row[\"label\"])\n",
    "        text  = str(row[\"text\"])\n",
    "\n",
    "        label_dir = subset_root / label\n",
    "        label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        tsv_path = label_dir / f\"{doc_id}.tsv\"\n",
    "        tsv_content = doc_to_tsv_str(text)\n",
    "\n",
    "        with open(tsv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tsv_content)\n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"{subset_name}: обработано {idx} документов...\")\n",
    "\n",
    "    print(f\"Готово: {subset_name} → {subset_root}\")\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "3002a054-6482-4350-8561-1f45d33ca0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T20:01:45.170947Z",
     "start_time": "2025-12-18T19:53:47.570122Z"
    }
   },
   "source": [
    "create_annotated_subset_from_df(train_std, \"train\")\n",
    "create_annotated_subset_from_df(test_std, \"test\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: обработано 0 документов...\n",
      "train: обработано 5000 документов...\n",
      "train: обработано 10000 документов...\n",
      "train: обработано 15000 документов...\n",
      "train: обработано 20000 документов...\n",
      "train: обработано 25000 документов...\n",
      "train: обработано 30000 документов...\n",
      "train: обработано 35000 документов...\n",
      "train: обработано 40000 документов...\n",
      "train: обработано 45000 документов...\n",
      "train: обработано 50000 документов...\n",
      "train: обработано 55000 документов...\n",
      "train: обработано 60000 документов...\n",
      "train: обработано 65000 документов...\n",
      "train: обработано 70000 документов...\n",
      "train: обработано 75000 документов...\n",
      "train: обработано 80000 документов...\n",
      "train: обработано 85000 документов...\n",
      "train: обработано 90000 документов...\n",
      "train: обработано 95000 документов...\n",
      "train: обработано 100000 документов...\n",
      "train: обработано 105000 документов...\n",
      "train: обработано 110000 документов...\n",
      "train: обработано 115000 документов...\n",
      "Готово: train → assets\\annotated-corpus\\train\n",
      "test: обработано 0 документов...\n",
      "test: обработано 5000 документов...\n",
      "Готово: test → assets\\annotated-corpus\\test\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "88484cd6-ad1a-4691-ac00-5cbb194db3ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T20:05:11.408607Z",
     "start_time": "2025-12-18T20:05:11.378569Z"
    }
   },
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "sample_label = random.choice([\"1\", \"2\", \"3\", \"4\"])\n",
    "sample_file = next((annotated_dir / \"train\" / sample_label).glob(\"*.tsv\"))\n",
    "\n",
    "print(\"Файл:\", sample_file)\n",
    "print(\"--- Содержимое фрагмента ---\")\n",
    "with open(sample_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(15):\n",
    "        print(f.readline().rstrip())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл: assets\\annotated-corpus\\train\\3\\000000.tsv\n",
      "--- Содержимое фрагмента ---\n",
      "Wall\twall\twall\n",
      "St\tst\tst\n",
      ".\t.\t.\n",
      "Bears\tbear\tbear\n",
      "Claw\tclaw\tclaw\n",
      "Back\tback\tback\n",
      "Into\tinto\tinto\n",
      "the\tthe\tthe\n",
      "Black\tblack\tblack\n",
      "(\t(\t(\n",
      "Reuters\treuter\treuters\n",
      ")\t)\t)\n",
      ".\t.\t.\n",
      "\n",
      "Reuters\treuter\treuters\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "4c485fef-1053-4544-b0bb-343c4f786bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T20:05:14.045350Z",
     "start_time": "2025-12-18T20:05:14.039306Z"
    }
   },
   "source": [
    "\n",
    "examples = [\n",
    "    (\"left\", \"n\", \"v\"),        # left / leave\n",
    "    (\"better\", \"a\", \"v\"),      # good / better\n",
    "    (\"meeting\", \"n\", \"v\"),     # meeting / meet\n",
    "    (\"building\", \"n\", \"v\"),    # building / build\n",
    "    (\"written\", \"a\", \"v\"),     # written / write\n",
    "    (\"lost\", \"a\", \"v\"),        # lost / lose\n",
    "    (\"bound\", \"a\", \"v\"),       # bound / bind\n",
    "    (\"closed\", \"a\", \"v\"),      # closed / close\n",
    "    (\"increased\", \"a\", \"v\"),   # increased / increase\n",
    "    (\"reduced\", \"a\", \"v\"),     # reduced / reduce\n",
    "    (\"produced\", \"a\", \"v\"),    # produced / produce\n",
    "    (\"developed\", \"a\", \"v\"),   # developed / develop\n",
    "    (\"broken\", \"a\", \"v\"),      # broken / break\n",
    "    (\"grown\", \"a\", \"v\"),       # grown / grow\n",
    "]\n",
    "\n",
    "\n",
    "for word, pos1, pos2 in examples:\n",
    "    lemma_1 = lemmatizer.lemmatize(word, pos=pos1)\n",
    "    lemma_2 = lemmatizer.lemmatize(word, pos=pos2)\n",
    "    print(f\"{word:8} → {pos1}: {lemma_1:8} | {pos2}: {lemma_2:8}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left     → n: left     | v: leave   \n",
      "better   → a: good     | v: better  \n",
      "meeting  → n: meeting  | v: meet    \n",
      "building → n: building | v: build   \n",
      "written  → a: written  | v: write   \n",
      "lost     → a: lost     | v: lose    \n",
      "bound    → a: bound    | v: bind    \n",
      "closed   → a: closed   | v: close   \n",
      "increased → a: increased | v: increase\n",
      "reduced  → a: reduced  | v: reduce  \n",
      "produced → a: produced | v: produce \n",
      "developed → a: developed | v: develop \n",
      "broken   → a: broken   | v: break   \n",
      "grown    → a: grown    | v: grow    \n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ea46b-530d-4161-8d72-4ff390398e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
