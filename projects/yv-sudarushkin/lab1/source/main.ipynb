{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet',\n",
    "          'test': 'plain_text/test-00000-of-00001.parquet',\n",
    "          'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:30:51.014087Z",
     "start_time": "2026-01-11T09:30:31.254107Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "COUNT_CORP = 5000\n",
    "df= pd.concat([df[df['label']==0].head(COUNT_CORP//2), df[df['label']==1].head(COUNT_CORP//2)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:30:51.230989Z",
     "start_time": "2026-01-11T09:30:51.212335Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:09.305123Z",
     "start_time": "2026-01-11T09:30:51.248993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])\n",
    "df_test= pd.concat([df_test[df_test['label']==0].head(COUNT_CORP//8), df_test[df_test['label']==1].head(COUNT_CORP//8)])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:09.326377Z",
     "start_time": "2026-01-11T09:31:09.318132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['label']==1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:09.370760Z",
     "start_time": "2026-01-11T09:31:09.340077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                    text  label\n",
       "12500  Zentropa has much in common with The Third Man...      1\n",
       "12501  Zentropa is the most original movie I've seen ...      1\n",
       "12502  Lars Von Trier is never backward in trying out...      1\n",
       "12503  *Contains spoilers due to me having to describ...      1\n",
       "12504  That was the first thing that sprang to mind a...      1\n",
       "...                                                  ...    ...\n",
       "14995  No rubbish - no where even near rubbish. Not a...      1\n",
       "14996  A weird, witty and wonderful depiction of fami...      1\n",
       "14997  This first two seasons of this comedy series w...      1\n",
       "14998  Typical 90's comedy, situational comedy simila...      1\n",
       "14999  this is one of the funniest shows i have ever ...      1\n",
       "\n",
       "[2500 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>Zentropa has much in common with The Third Man...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>Zentropa is the most original movie I've seen ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>Lars Von Trier is never backward in trying out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>*Contains spoilers due to me having to describ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>That was the first thing that sprang to mind a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>No rubbish - no where even near rubbish. Not a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>A weird, witty and wonderful depiction of fami...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>This first two seasons of this comedy series w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>Typical 90's comedy, situational comedy simila...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>this is one of the funniest shows i have ever ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:25.010183Z",
     "start_time": "2026-01-11T09:31:09.433298Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# Скачиваем необходимые ресурсы NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:25.679243Z",
     "start_time": "2026-01-11T09:31:25.040205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yaros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yaros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:34.628756Z",
     "start_time": "2026-01-11T09:31:25.725191Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from sentence_segmenter import SentenceSegmenter, test_problem_cases\n",
    "\n",
    "\n",
    "test_problem_cases()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:34.767939Z",
     "start_time": "2026-01-11T09:31:34.674808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST CASE 1 ===\n",
      "Input: Hello Dr. Smith! How are you? I'm fine.\n",
      "Original text: Hello Dr. Smith! How are you? I'm fine.\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "Pattern 'common_abbr' matched: 'Dr.'\n",
      "  Position: 6-9\n",
      "  Context: ...ello Dr. Smit...\n",
      "  Dots in match: 1\n",
      "\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 8: ...Hello Dr. Smith! H...\n",
      "Dot at position 38: ...? I'm fine....\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: Hello Dr‹DOT› Smith! How are you? I'm fine.\n",
      "Replacements: {'Dr.': 'Dr‹DOT›'}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 19: '!' in context 'Smith! How'\n",
      "  Should split: True\n",
      "\n",
      "Position 32: '?' in context 'e you? I'm'\n",
      "  Should split: True\n",
      "\n",
      "Position 42: '.' in context ' fine.'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: Hello Dr. Smith!\n",
      "Sentence 2: How are you?\n",
      "Sentence 3: I'm fine.\n",
      "\n",
      "Output: ['Hello Dr. Smith!', 'How are you?', \"I'm fine.\"]\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== TEST CASE 2 ===\n",
      "Input: The meeting is at 2:30 p.m. in room 3.14. Don't be late!\n",
      "Original text: The meeting is at 2:30 p.m. in room 3.14. Don't be late!\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "Pattern 'time' matched: '2:30 p.m.'\n",
      "  Position: 18-27\n",
      "  Context: ...s at 2:30 p.m. in r...\n",
      "  Dots in match: 2\n",
      "\n",
      "Pattern 'decimal' matched: '3.14'\n",
      "  Position: 36-40\n",
      "  Context: ...room 3.14. Don...\n",
      "  Dots in match: 1\n",
      "\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 24: ... at 2:30 p.m. in roo...\n",
      "Dot at position 26: ...t 2:30 p.m. in room ...\n",
      "Dot at position 37: ... in room 3.14. Don't...\n",
      "Dot at position 40: ... room 3.14. Don't be...\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: The meeting is at 2:30 p‹DOT›m‹DOT› in room 3‹DOT›14. Don't be late!\n",
      "Replacements: {'2:30 p.m.': '2:30 p‹DOT›m‹DOT›', '3.14': '3‹DOT›14'}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 52: '.' in context 'OT›14. Don'\n",
      "  Should split: True\n",
      "\n",
      "Position 67: '!' in context ' late!'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: The meeting is at 2:30 p.m. in room 3.14.\n",
      "Sentence 2: Don't be late!\n",
      "\n",
      "Output: ['The meeting is at 2:30 p.m. in room 3.14.', \"Don't be late!\"]\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== TEST CASE 3 ===\n",
      "Input: She works at ABC Inc. Her email is test@example.com.\n",
      "Original text: She works at ABC Inc. Her email is test@example.com.\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "Pattern 'email' matched: 'test@example.com'\n",
      "  Position: 35-51\n",
      "  Context: ...l is test@example.com....\n",
      "  Dots in match: 1\n",
      "\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 20: ...at ABC Inc. Her emai...\n",
      "Dot at position 47: ...st@example.com....\n",
      "Dot at position 51: ...xample.com....\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: She works at ABC Inc. Her email is test@example‹DOT›com.\n",
      "Replacements: {'test@example.com': 'test@example‹DOT›com'}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 20: '.' in context 'C Inc. Her'\n",
      "  Should split: True\n",
      "\n",
      "Position 55: '.' in context 'T›com.'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: She works at ABC Inc.\n",
      "Sentence 2: Her email is test@example.com.\n",
      "\n",
      "Output: ['She works at ABC Inc.', 'Her email is test@example.com.']\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== TEST CASE 4 ===\n",
      "Input: Visit https://example.com. It's a great website!\n",
      "Original text: Visit https://example.com. It's a great website!\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "Pattern 'url' matched: 'https://example.com'\n",
      "  Position: 6-25\n",
      "  Context: ...isit https://example.com. It'...\n",
      "  Dots in match: 1\n",
      "\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 21: ...://example.com. It's...\n",
      "Dot at position 25: ...xample.com. It's a g...\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: Visit https://example‹DOT›com. It's a great website!\n",
      "Replacements: {'https://example.com': 'https://example‹DOT›com'}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 29: '.' in context 'T›com. It''\n",
      "  Should split: True\n",
      "\n",
      "Position 51: '!' in context 'bsite!'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: Visit https://example.com.\n",
      "Sentence 2: It's a great website!\n",
      "\n",
      "Output: ['Visit https://example.com.', \"It's a great website!\"]\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== TEST CASE 5 ===\n",
      "Input: I like apples, oranges, etc. But I don't like bananas.\n",
      "Original text: I like apples, oranges, etc. But I don't like bananas.\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 27: ...anges, etc. But I do...\n",
      "Dot at position 53: ...ke bananas....\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: I like apples, oranges, etc. But I don't like bananas.\n",
      "Replacements: {}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 27: '.' in context ', etc. But'\n",
      "  Should split: True\n",
      "\n",
      "Position 53: '.' in context 'nanas.'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: I like apples, oranges, etc.\n",
      "Sentence 2: But I don't like bananas.\n",
      "\n",
      "Output: ['I like apples, oranges, etc.', \"But I don't like bananas.\"]\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== TEST CASE 6 ===\n",
      "Input: The price is $19.99. It's a good deal!\n",
      "Original text: The price is $19.99. It's a good deal!\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "Pattern 'decimal' matched: '19.99'\n",
      "  Position: 14-19\n",
      "  Context: ... is $19.99. It'...\n",
      "  Dots in match: 1\n",
      "\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 16: ...ice is $19.99. It's ...\n",
      "Dot at position 19: ... is $19.99. It's a g...\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: The price is $19‹DOT›99. It's a good deal!\n",
      "Replacements: {'19.99': '19‹DOT›99'}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 23: '.' in context 'OT›99. It''\n",
      "  Should split: True\n",
      "\n",
      "Position 41: '!' in context ' deal!'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: The price is $19.99.\n",
      "Sentence 2: It's a good deal!\n",
      "\n",
      "Output: ['The price is $19.99.', \"It's a good deal!\"]\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== TEST CASE 7 ===\n",
      "Input: Check version v2.1.3. It fixes many bugs.\n",
      "Original text: Check version v2.1.3. It fixes many bugs.\n",
      "\n",
      "=== DEBUG PROTECTION PATTERNS ===\n",
      "Pattern 'version' matched: 'v2.1.3'\n",
      "  Position: 14-20\n",
      "  Context: ...sion v2.1.3. It ...\n",
      "  Dots in match: 2\n",
      "\n",
      "=== ALL DOTS IN TEXT ===\n",
      "Dot at position 16: ...version v2.1.3. It f...\n",
      "Dot at position 18: ...rsion v2.1.3. It fix...\n",
      "Dot at position 20: ...ion v2.1.3. It fixes...\n",
      "Dot at position 40: ... many bugs....\n",
      "\n",
      "=== AFTER PROTECTION ===\n",
      "Protected text: Check version v2‹DOT›1‹DOT›3. It fixes many bugs.\n",
      "Replacements: {'v2.1.3': 'v2‹DOT›1‹DOT›3'}\n",
      "\n",
      "=== SENTENCE SPLITTING ===\n",
      "Position 28: '.' in context 'DOT›3. It '\n",
      "  Should split: True\n",
      "\n",
      "Position 48: '.' in context ' bugs.'\n",
      "  Should split: True\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Sentence 1: Check version v2.1.3.\n",
      "Sentence 2: It fixes many bugs.\n",
      "\n",
      "Output: ['Check version v2.1.3.', 'It fixes many bugs.']\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "segmentor = SentenceSegmenter(debug=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:34.837051Z",
     "start_time": "2026-01-11T09:31:34.777953Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "cur = 6\n",
    "for i, test_case in enumerate(df['text'][cur:cur+1], 1):\n",
    "    print(f\"=== TEST CASE {i} ===\")\n",
    "    print(f\"Input: {test_case}\")\n",
    "    sentences = segmentor.segment_text(test_case)\n",
    "    print(f\"Output: {sentences}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:34.911556Z",
     "start_time": "2026-01-11T09:31:34.858163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST CASE 1 ===\n",
      "Input: Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\n",
      "Output: ['Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography.', \"I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi.\", 'I could write a whole list of factual errors, but it would go on for pages.', 'In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves.', 'If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film.', 'The filmmakers tried hard, but the movie seems awfully sloppy to me.']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "from text_tokenizer import TextTokenizer\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TextTokenizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.segmenter = SentenceSegmenter()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stopwords.update([\"the\", \"film\", \"movie\", \"like\"])\n",
    "\n",
    "    def process_sentence(self, sentence):\n",
    "        \"\"\"Обработка одного предложения\"\"\"\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        tokens = [word for word in tokens if word not in self.stopwords and len(word) > 2]\n",
    "        processed = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Для слов применяем стемминг и лемматизацию\n",
    "            if re.match(r'\\b[a-zA-Z]+\\b', token):\n",
    "                stem = self.stemmer.stem(token.lower())\n",
    "                lemma = self.lemmatizer.lemmatize(token.lower())\n",
    "            else:\n",
    "                # Для не-слов оставляем как есть\n",
    "                stem = token\n",
    "                lemma = token\n",
    "\n",
    "            processed.append((token, stem, lemma))\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def process_text(self, text):\n",
    "        \"\"\"Обработка всего текста\"\"\"\n",
    "        sentences = self.segmenter.segment_text(text)\n",
    "        all_processed = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            processed_sentence = self.process_sentence(sentence)\n",
    "            all_processed.extend(processed_sentence)\n",
    "            # Добавляем пустую строку между предложениями\n",
    "            all_processed.append(('', '', ''))\n",
    "\n",
    "        return all_processed[:-1]  # Убираем последнюю пустую строку"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:31:34.997486Z",
     "start_time": "2026-01-11T09:31:34.929082Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "def create_annotation_files(df, split_name='train'):\n",
    "    \"\"\"Создание аннотированных файлов для датасета\"\"\"\n",
    "    processor = TextProcessor()\n",
    "    base_path = f'../assets/annotated-corpus/{split_name}'\n",
    "\n",
    "    # Создаем директории\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = 'pos' if row['label'] == 1 else 'neg'\n",
    "\n",
    "        # Создаем поддиректорию для класса\n",
    "        label_dir = os.path.join(base_path, label)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        # Обрабатываем текст\n",
    "        annotations = processor.process_text(text)\n",
    "\n",
    "        # Сохраняем в TSV\n",
    "        file_path = os.path.join(label_dir, f\"{idx}.tsv\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            for token, stem, lemma in annotations:\n",
    "                if token == '' and stem == '' and lemma == '':\n",
    "                    f.write('\\n')  # Разделитель между предложениями\n",
    "                else:\n",
    "                    f.write(f\"{token}\\t{stem}\\t{lemma}\\n\")\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Обработано {idx} документов...\")\n",
    "\n",
    "    print(f\"Аннотации сохранены в {base_path}\")\n",
    "\n",
    "# Обрабатываем тренировочные данные\n",
    "create_annotation_files(df, 'train')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:32:16.100025Z",
     "start_time": "2026-01-11T09:31:35.031413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 0 документов...\n",
      "Обработано 100 документов...\n",
      "Обработано 200 документов...\n",
      "Обработано 300 документов...\n",
      "Обработано 400 документов...\n",
      "Обработано 500 документов...\n",
      "Обработано 600 документов...\n",
      "Обработано 700 документов...\n",
      "Обработано 800 документов...\n",
      "Обработано 900 документов...\n",
      "Обработано 1000 документов...\n",
      "Обработано 1100 документов...\n",
      "Обработано 1200 документов...\n",
      "Обработано 1300 документов...\n",
      "Обработано 1400 документов...\n",
      "Обработано 1500 документов...\n",
      "Обработано 1600 документов...\n",
      "Обработано 1700 документов...\n",
      "Обработано 1800 документов...\n",
      "Обработано 1900 документов...\n",
      "Обработано 2000 документов...\n",
      "Обработано 2100 документов...\n",
      "Обработано 2200 документов...\n",
      "Обработано 2300 документов...\n",
      "Обработано 2400 документов...\n",
      "Обработано 12500 документов...\n",
      "Обработано 12600 документов...\n",
      "Обработано 12700 документов...\n",
      "Обработано 12800 документов...\n",
      "Обработано 12900 документов...\n",
      "Обработано 13000 документов...\n",
      "Обработано 13100 документов...\n",
      "Обработано 13200 документов...\n",
      "Обработано 13300 документов...\n",
      "Обработано 13400 документов...\n",
      "Обработано 13500 документов...\n",
      "Обработано 13600 документов...\n",
      "Обработано 13700 документов...\n",
      "Обработано 13800 документов...\n",
      "Обработано 13900 документов...\n",
      "Обработано 14000 документов...\n",
      "Обработано 14100 документов...\n",
      "Обработано 14200 документов...\n",
      "Обработано 14300 документов...\n",
      "Обработано 14400 документов...\n",
      "Обработано 14500 документов...\n",
      "Обработано 14600 документов...\n",
      "Обработано 14700 документов...\n",
      "Обработано 14800 документов...\n",
      "Обработано 14900 документов...\n",
      "Аннотации сохранены в ../assets/annotated-corpus/train\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "create_annotation_files(df_test, 'test')",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:32:24.898315Z",
     "start_time": "2026-01-11T09:32:16.118046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 0 документов...\n",
      "Обработано 100 документов...\n",
      "Обработано 200 документов...\n",
      "Обработано 300 документов...\n",
      "Обработано 400 документов...\n",
      "Обработано 500 документов...\n",
      "Обработано 600 документов...\n",
      "Обработано 12500 документов...\n",
      "Обработано 12600 документов...\n",
      "Обработано 12700 документов...\n",
      "Обработано 12800 документов...\n",
      "Обработано 12900 документов...\n",
      "Обработано 13000 документов...\n",
      "Обработано 13100 документов...\n",
      "Аннотации сохранены в ../assets/annotated-corpus/test\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
