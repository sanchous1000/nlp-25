{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2 - Text Vectorization (Векторизация текста)\n",
        "\n",
        "This notebook demonstrates the complete text vectorization pipeline:\n",
        "1. Task 1 (Optional): Build token dictionary and term-document matrix\n",
        "2. Task 2 (Optional): Basic vectorization methods\n",
        "3. Task 3: Neural network vectorization (Word2Vec)\n",
        "4. Task 4: Cosine similarity demonstrations\n",
        "5. Task 5 (Optional): Dimensionality reduction\n",
        "6. Task 6 (Optional): Compare methods\n",
        "7. Task 7: Document vectorization pipeline\n",
        "8. Task 8: Vectorize test set and save in TSV format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add source directory to path\n",
        "sys.path.append('source')\n",
        "\n",
        "from source.data_loader import load_corpus, get_all_tokens, get_sentences_as_tokens\n",
        "from source.token_dictionary import TokenDictionary\n",
        "from source.basic_vectorization import BasicVectorizer\n",
        "from source.neural_vectorization import NeuralVectorizer\n",
        "from source.cosine_similarity import cosine_distance, demonstrate_semantic_similarity, find_most_similar_words\n",
        "from source.document_vectorizer import DocumentVectorizer\n",
        "from source.dimensionality_reduction import DimensionalityReducer\n",
        "\n",
        "# Set up plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except:\n",
        "        pass\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 (Optional): Build Token Dictionary and Term-Document Matrix\n",
        "\n",
        "Load training data and build token dictionary with frequencies and term-document matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 121884/121884 [00:35<00:00, 3472.75files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 121884 training documents\n",
            "Total tokens: 3964419\n",
            "\n",
            "Building token dictionary...\n",
            "Built dictionary with 35311 tokens\n",
            "Total tokens processed: 2466038\n",
            "Tokens filtered out (frequency < 2): 19589\n",
            "\n",
            "Building term-document matrix...\n",
            "Built term-document matrix: (35311, 121884)\n",
            "Matrix density: 0.0005\n",
            "Saved dictionary to assets/dictionaries/token_dictionary.json\n",
            "Saved term-document matrix to assets/matrices/term_document_matrix.pkl\n",
            "\n",
            "Vocabulary size: 35311\n",
            "Dictionary and matrix saved!\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "lab1_corpus_dir = \"../lab1/assets/annotated_corpus\"\n",
        "print(\"Loading training data...\")\n",
        "train_docs, train_labels, train_ids = load_corpus(lab1_corpus_dir, split='train')\n",
        "train_tokens = get_all_tokens(train_docs, use_lemma=True)\n",
        "\n",
        "print(f\"Loaded {len(train_docs)} training documents\")\n",
        "print(f\"Total tokens: {sum(len(doc) for doc in train_tokens)}\")\n",
        "\n",
        "# Build token dictionary\n",
        "print(\"\\nBuilding token dictionary...\")\n",
        "token_dict = TokenDictionary(min_frequency=2, remove_stopwords=True, remove_punctuation=True)\n",
        "token_dict.build_dictionary(train_tokens)\n",
        "\n",
        "# Build term-document matrix\n",
        "print(\"\\nBuilding term-document matrix...\")\n",
        "token_dict.build_term_document_matrix(train_tokens)\n",
        "\n",
        "# Save results\n",
        "os.makedirs('assets/dictionaries', exist_ok=True)\n",
        "os.makedirs('assets/matrices', exist_ok=True)\n",
        "token_dict.save_dictionary('assets/dictionaries/token_dictionary.json')\n",
        "token_dict.save_term_document_matrix('assets/matrices/term_document_matrix.pkl')\n",
        "\n",
        "print(f\"\\nVocabulary size: {token_dict.get_vocab_size()}\")\n",
        "print(\"Dictionary and matrix saved!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Neural Network Vectorization (Word2Vec)\n",
        "\n",
        "Train Word2Vec model on training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sentences for training: 121884\n",
            "Training word2vec model...\n",
            "  Vector size: 100\n",
            "  Window: 5\n",
            "  Min count: 2\n",
            "  Training algorithm: CBOW\n",
            "  Number of documents: 121884\n",
            "Training completed! Vocabulary size: 43609\n",
            "Saved model to assets/models/word2vec_model.model\n",
            "\n",
            "Model trained! Vocabulary size: 43609\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for Word2Vec (list of sentences)\n",
        "train_sentences = get_sentences_as_tokens(train_docs, use_lemma=True)\n",
        "train_sentences_flat = []\n",
        "for doc in train_sentences:\n",
        "    train_sentences_flat.extend(doc)\n",
        "\n",
        "print(f\"Total sentences for training: {len(train_sentences_flat)}\")\n",
        "\n",
        "# Train Word2Vec model\n",
        "neural_model = NeuralVectorizer(\n",
        "    model_type='word2vec',\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=4,\n",
        "    sg=0  # CBOW\n",
        ")\n",
        "\n",
        "neural_model.train(train_sentences_flat, epochs=10)\n",
        "\n",
        "# Save model\n",
        "os.makedirs('assets/models', exist_ok=True)\n",
        "neural_model.save('assets/models/word2vec_model.model')\n",
        "\n",
        "print(f\"\\nModel trained! Vocabulary size: {len(neural_model.wv)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Demonstrate Cosine Similarity\n",
        "\n",
        "Show that semantically close words have smaller cosine distance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Base word: 'sport'\n",
            "============================================================\n",
            "\n",
            "Similar words:\n",
            "  athlete              - distance: 0.3720\n",
            "  player               - distance: 0.6063\n",
            "  game                 - distance: 0.8501\n",
            "  match                - distance: 0.9130\n",
            "\n",
            "Related words:\n",
            "  football             - distance: 0.4490\n",
            "  competition          - distance: 0.5827\n",
            "  basketball           - distance: 0.5923\n",
            "  team                 - distance: 0.6785\n",
            "\n",
            "Distant words:\n",
            "  computer             - distance: 0.7200\n",
            "  philosophy           - distance: 0.7905\n",
            "  mathematics          - distance: 0.8618\n",
            "  sentence             - distance: 1.0975\n",
            "\n",
            "============================================================\n",
            "Base word: 'technology'\n",
            "============================================================\n",
            "\n",
            "Similar words:\n",
            "  software             - distance: 0.3881\n",
            "  digital              - distance: 0.4694\n",
            "  computer             - distance: 0.5214\n",
            "  electronic           - distance: 0.5690\n",
            "\n",
            "Related words:\n",
            "  device               - distance: 0.4066\n",
            "  network              - distance: 0.4311\n",
            "  system               - distance: 0.4526\n",
            "  internet             - distance: 0.6902\n",
            "\n",
            "Distant words:\n",
            "  sport                - distance: 0.6444\n",
            "  music                - distance: 0.6935\n",
            "  cooking              - distance: 0.8147\n",
            "  animal               - distance: 0.8564\n",
            "\n",
            "============================================================\n",
            "Base word: 'business'\n",
            "============================================================\n",
            "\n",
            "Similar words:\n",
            "  market               - distance: 0.4026\n",
            "  company              - distance: 0.5483\n",
            "  trade                - distance: 0.7224\n",
            "  commerce             - distance: 0.7413\n",
            "\n",
            "Related words:\n",
            "  investment           - distance: 0.4386\n",
            "  economy              - distance: 0.5917\n",
            "  profit               - distance: 0.6960\n",
            "  finance              - distance: 0.7017\n",
            "\n",
            "Distant words:\n",
            "  art                  - distance: 0.7610\n",
            "  science              - distance: 0.8109\n",
            "  nature               - distance: 0.8370\n",
            "  philosophy           - distance: 0.8381\n"
          ]
        }
      ],
      "source": [
        "# Define test words for demonstration\n",
        "test_words = {\n",
        "    'sport': {\n",
        "        'similar': ['game', 'athlete', 'player', 'match'],\n",
        "        'related': ['team', 'competition', 'football', 'basketball'],\n",
        "        'distant': ['computer', 'philosophy', 'mathematics', 'sentence']\n",
        "    },\n",
        "    'technology': {\n",
        "        'similar': ['computer', 'software', 'digital', 'electronic'],\n",
        "        'related': ['internet', 'device', 'system', 'network'],\n",
        "        'distant': ['sport', 'animal', 'cooking', 'music']\n",
        "    },\n",
        "    'business': {\n",
        "        'similar': ['company', 'market', 'trade', 'commerce'],\n",
        "        'related': ['economy', 'finance', 'investment', 'profit'],\n",
        "        'distant': ['nature', 'art', 'science', 'philosophy']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Demonstrate semantic similarity\n",
        "results = demonstrate_semantic_similarity(neural_model, test_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Most similar words to 'sport':\n",
            "  art                  - similarity: 0.6502\n",
            "  athlete              - similarity: 0.6280\n",
            "  talent               - similarity: 0.6080\n",
            "  wine                 - similarity: 0.5780\n",
            "  baseball             - similarity: 0.5728\n",
            "  spirit               - similarity: 0.5691\n",
            "  elite                - similarity: 0.5526\n",
            "  arena                - similarity: 0.5513\n",
            "  football             - similarity: 0.5510\n",
            "  science              - similarity: 0.5482\n"
          ]
        }
      ],
      "source": [
        "# Find most similar words using built-in method\n",
        "test_word = 'sport'\n",
        "similar_words = find_most_similar_words(neural_model, test_word, top_n=10)\n",
        "\n",
        "print(f\"\\nMost similar words to '{test_word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"  {word:20s} - similarity: {similarity:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 7: Document Vectorization Pipeline\n",
        "\n",
        "Implement complete document vectorization: sentences → tokens → token vectors → sentence vectors → document vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segmented sentences:\n",
            "  Sentence 1: ['the', 'technology', 'company', 'announced', 'new', 'software', 'products']\n",
            "  Sentence 2: ['the', 'digital', 'market', 'is', 'growing', 'rapidly']\n",
            "\n",
            "Document vector shape: (100,)\n",
            "Document vector (first 10 components): [-0.19103011 -0.69757634 -0.18636397 -0.3347658  -0.3205855  -0.53389674\n",
            "  1.3607087  -1.139126    0.7254089  -0.41075262]\n"
          ]
        }
      ],
      "source": [
        "# Initialize document vectorizer\n",
        "doc_vectorizer = DocumentVectorizer(neural_model)\n",
        "\n",
        "# Example: Vectorize a sample document\n",
        "sample_text = \"The technology company announced new software products. The digital market is growing rapidly.\"\n",
        "\n",
        "# Get sentences\n",
        "sentences = doc_vectorizer.segment_text(sample_text)\n",
        "print(\"Segmented sentences:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"  Sentence {i+1}: {sent}\")\n",
        "\n",
        "# Vectorize document\n",
        "doc_vector = doc_vectorizer.vectorize_document(sample_text, use_tfidf_weights=False)\n",
        "print(f\"\\nDocument vector shape: {doc_vector.shape}\")\n",
        "print(f\"Document vector (first 10 components): {doc_vector[:10]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 8: Vectorize Test Set and Save in TSV Format\n",
        "\n",
        "Vectorize all test documents and save embeddings in TSV format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 7600/7600 [00:02<00:00, 3157.70files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 7600 test documents\n",
            "\n",
            "Vectorizing test documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7600/7600 [00:03<00:00, 2489.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated embeddings shape: (7600, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "test_docs, test_labels, test_ids = load_corpus(lab1_corpus_dir, split='test')\n",
        "test_sentences = get_sentences_as_tokens(test_docs, use_lemma=True)\n",
        "\n",
        "print(f\"Loaded {len(test_docs)} test documents\")\n",
        "\n",
        "# Vectorize test documents\n",
        "print(\"\\nVectorizing test documents...\")\n",
        "embeddings = []\n",
        "valid_doc_ids = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "for doc_sentences, doc_id in tqdm(zip(test_sentences, test_ids), total=len(test_docs)):\n",
        "    doc_vector = doc_vectorizer.vectorize_document_from_tokens(\n",
        "        doc_sentences,\n",
        "        use_tfidf_weights=False\n",
        "    )\n",
        "    embeddings.append(doc_vector)\n",
        "    valid_doc_ids.append(doc_id)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "print(f\"\\nGenerated embeddings shape: {embeddings.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved embeddings to assets/embeddings/test_embeddings.tsv\n",
            "Total documents: 7600\n",
            "Embedding dimension: 100\n",
            "\n",
            "First 3 lines of output file:\n",
            "  Doc ID: 1003, First 5 components: ['-0.775902', '-0.199960', '0.406633', '-0.609266', '-0.205569']\n",
            "  Doc ID: 1017, First 5 components: ['1.457490', '-4.045797', '1.432831', '-2.652643', '0.766079']\n",
            "  Doc ID: 1021, First 5 components: ['-0.218004', '-0.991015', '-0.162545', '-0.767218', '-0.116455']\n"
          ]
        }
      ],
      "source": [
        "# Save embeddings in TSV format\n",
        "os.makedirs('assets/embeddings', exist_ok=True)\n",
        "output_file = 'assets/embeddings/test_embeddings.tsv'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for doc_id, embedding in zip(valid_doc_ids, embeddings):\n",
        "        embedding_str = '\\t'.join([f\"{val:.6f}\" for val in embedding])\n",
        "        f.write(f\"{doc_id}\\t{embedding_str}\\n\")\n",
        "\n",
        "print(f\"Saved embeddings to {output_file}\")\n",
        "print(f\"Total documents: {len(valid_doc_ids)}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "# Show first few lines\n",
        "print(\"\\nFirst 3 lines of output file:\")\n",
        "with open(output_file, 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 3:\n",
        "            parts = line.strip().split('\\t')\n",
        "            print(f\"  Doc ID: {parts[0]}, First 5 components: {parts[1:6]}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
